---
date: 01-02-02
title: MAP 2.2
categories:
  - MAP-2
description:  Information is documented about the operational context in which the AI system will be deployed (e.g., human-AI teaming, etc.) and how output will be utilized and overseen by humans.
type: Map
order_number: 2
---

{::options parse_block_html="true" /}


<details>
<summary markdown="span">**About**</summary>
<br>
Once deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors, including developing systems in highly-controlled environments that differ considerably from the deployment context. Regular stakeholder engagement and feedback can provide enhanced contextual awareness about how an AI system may interact in its real-world setting. Example practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions.
 
</details>

<details>
<summary markdown="span">**Actions**</summary>

* Extend documentation beyond system and task requirements to include possible risks due to deployment contexts and human-AI configurations. 
* Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether users can correctly comprehend system outputs or results.
* Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data.
* Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment decisions.

</details>

<details>
<summary markdown="span">**Transparency and Documentation**</summary>
<br>
**Transparency Considerations – Key Questions: MAP 2.2**
- Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly?
- To what extent is the output of each component appropriate for the operational context?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals     impacted by use of the AI system?
- Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment)
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?

**AI Transparency Resources: MAP 2.2**
- Datasheets for Datasets
- WEF Model AI Governance Framework Assessment 2020
- Companion to the Model AI Governance Framework- 2020
- ATARC Model Transparency Assessment (WD) – 2020
- Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020

</details>

<details>
<summary markdown="span">**References**</summary>    
<br>
**Context of use**

International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. [URL](https://www.iso.org/standard/77520.html)

National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. [URL](https://doi.org/10.6028/NIST.HB.161)

**Human-machine interaction**

Smith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.

Warden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100

Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. [URL](https://nap.nationalacademies.org/catalog/26355/human-ai-teaming-state-of-the-art-and-research-needs)

Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law & Security Review 45 (26 Apr. 2021). [URL](https://dx.doi.org/10.2139/ssrn.3921216)

Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). [URL](https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html)

Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. [URL](https://doi.org/10.1145/3411764.3445315)

Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). [URL](https://doi.org/10.1038/s41746-021-00385-9)

Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. [URL](https://doi.org/10.1145/3449287)

</details>
  
