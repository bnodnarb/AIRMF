<!DOCTYPE html>
<link rel="stylesheet" href="https://pages.nist.gov/nist-header-footer/css/nist-combined.css">
      <script src="https://pages.nist.gov/nist-header-footer/js/jquery-1.9.0.min.js" type="text/javascript" defer="defer"></script>
      <script src="https://pages.nist.gov/nist-header-footer/js/nist-header-footer.js" type="text/javascript" defer="defer"></script>


<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="/RMF/css/screen.css">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">

		

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>MAP 1.1 | AI RMF Playbook</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="MAP 1.1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Setting in which the AI system will be deployed, the specific set of users along with their expectations, and impacts of system use are understood and documented as appropriate." />
<meta property="og:description" content="Setting in which the AI system will be deployed, the specific set of users along with their expectations, and impacts of system use are understood and documented as appropriate." />
<link rel="canonical" href="http://localhost:4000/RMF/mytest.html" />
<meta property="og:url" content="http://localhost:4000/RMF/mytest.html" />
<meta property="og:site_name" content="AI RMF Playbook" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2012-01-16T00:00:00+00:00" />
<script type="application/ld+json">
{"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/RMF/NISTlogo.png"}},"headline":"MAP 1.1","dateModified":"2012-01-16T00:00:00+00:00","datePublished":"2012-01-16T00:00:00+00:00","@type":"BlogPosting","url":"http://localhost:4000/RMF/mytest.html","description":"Setting in which the AI system will be deployed, the specific set of users along with their expectations, and impacts of system use are understood and documented as appropriate.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/RMF/mytest.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/RMF/feed.xml" title="AI RMF Playbook" />

	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
                                        <nav>
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/" >HOME</a>
                         </button>
                    
                
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/govern/" >GOVERN</a>
                         </button>
                    
                
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/map/" >MAP</a>
                         </button>
                    
                
	
	
		
                
                     <a class="">MEASURE</a>
                
	
	
		
                
                     <a class="">MANAGE</a>
                
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/terms/" >TERMS</a>
                         </button>
                    
                
	
	
		
                
                    
       			 <button  class="navbtn scale"><a class="">&nbsp;</a></button>
        		 <button  class="navbtn scale"><a href="/RMF/about/">ABOUT</a></button>
                    
                
	
</nav>


				</section>
				<section class="hero_search">
					<h1>AI Risk Managment Framework Playbook</h1>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				
<p>Setting in which the AI system will be deployed, the specific set of users along with their expectations, and impacts of system use are understood and documented as appropriate.</p>

<details>
  <summary><span style="background-color:#EBEAE9">What is this about?</span></summary>

  <p>Context includes the intended and actual setting in which it is deployed, the specific set of users, operators or subjects along with their expectations, concept of operations, intended purposes and impacts of system use, the necessary requirements to ensure the system can be optimally deployed and operated, potential negative impacts to individuals, groups, communities, organizations, and society and any other system or context specifications, or legal requirements, or impacts to the environment. Context may also include unintended, downstream, off-label, or other unforeseen scopes of application.</p>

  <p>A fundamental step to mapping context is having a broad and appropriate set of skills and perspectives at the table. Within an organization this means team composition- demographic, disciplinary, experiential- that can enhance creativity and the consideration of risks. Organizational management should recognize the importance of diversity beyond its business case. By providing license for all team members to freely engage in critical inquiry, management can work to ensure that pervasive institutional biases are not inadvertently squashing creativity. This commitment to diverse and inclusive teaming increases the ability of an organization to broaden their contextual perspectives, check their assumptions about context of use, recognize when systems are not functional within and out of the intended context, and identify constraints in real world applications that may lead to harmful impacts.</p>

</details>

<details>
  <summary><span style="background-color:#EBEAE9">What steps can be taken to implement this?</span></summary>

  <ul>
    <li>Plan and document the composition of AI design and development teams to reflect inter-disciplinary roles, competencies, skills and capacity for AI efforts; and ensure that team membership incorporates demographic diversity and broad domain expertise.</li>
    <li>Gain and maintain familiarity with the complexities and interdependencies of deployed AI systems; terminology and concepts from disciplines outside of AI practice such as the law, sociology, psychology, public policy, and systems design and engineering.</li>
    <li>Maintain awareness of industry and technical standards and appropriate legal standards.</li>
    <li>Track, document or inventory the organization’s AI systems, including existing systems and third-party entities associated with AI systems.</li>
    <li>Gain and maintain awareness for how to scientifically evaulate claims about AI system performance and benefits before launching into system design and development to enable adherence to responsible practices.</li>
    <li>Define and document the task, purpose, minimum functionality, and benefits of the AI system, and consider whether the project is worth pursuing.</li>
    <li>Define the context of use, including operational environment; impacts to individuals, groups, communities, organizations, and society; user characteristics; task; and social environment; determine the user and organizational requirements, including business requirements, user requirements, and technical requirements.</li>
    <li>Identify human-AI interaction and/or roles, such as whether the application will support human decision making, replace a human, and make predictions; plan for risks related to these configurations; and document requirements, roles, and responsibilities for human oversight of deployed systems.</li>
  </ul>

</details>

<details>
  <summary>What are the transparency and documentation considerations?</summary>

  <p><strong>Transparency Considerations – Key Questions: MAP 1.1</strong></p>
  <ul>
    <li>Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?&lt;/li&gt;</li>
    <li>Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?</li>
    <li>Who is accountable for the ethical considerations during all stages of the AI lifecycle?</li>
    <li>Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap that needed to be filled?&lt;/li&gt;</li>
    <li>How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?</li>
  </ul>

  <p><strong>AI Transparency Resources: MAP 1.1</strong></p>
  <ul>
    <li>Datasheets for Datasets&lt;/li&gt;</li>
    <li>GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities&lt;/li&gt;</li>
    <li>“Stakeholders in Explainable AI,” Sep. 2018, [Online]. <a href="http://arxiv.org/abs/1810.00184">link</a></li>
  </ul>

</details>

<details>
  <summary>What are some informative references?</summary>

  <p>For more information about:</p>

  <p><strong>Socio-technical systems</strong>    <br />
Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. <em>In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT’19). Association for Computing Machinery, New York, NY, USA, 59–68.</em> <a href="https://doi.org/10.1145/3287560.3287598">link</a></p>

  <p><strong>Problem formulation</strong>  <br />
Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. <em>Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702.</em> <a href="https://doi.org/10.1016/j.artint.2021.103555">link</a></p>

  <p>Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. <em>In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT’19). Association for Computing Machinery, New York, NY, USA, 39–48.</em> <a href="https://doi.org/10.1145/3287560.3287567">link</a></p>

  <p><strong>Context mapping</strong>  <br />
Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. <em>Joint Research Centre (European Commission).</em> Retrieved from <a href="https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en">link</a></p>

  <p>Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. <em>arXiv:2004.13676.</em>  Retrieved from <a href="https://arxiv.org/abs/2004.13676">link</a></p>

  <p>Social Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change Projects (Draft v2.0). Retrieved from <a href="https://www.alnap.org/system/files/content/resource/files/main/Draft%20SIMLab%20Context%20Analysis%20Framework%20v2.0.pdf">link</a></p>

  <p>Solon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. <em>Commun. ACM 64, 7 (July 2021), 30–32.</em> <a href="https://doi.org/10.1145/3466612">link</a></p>

  <p><strong>Identification of harms</strong>  <br />
Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. <em>arXiv:1901.10002.</em> Retrieved from <a href="https://arxiv.org/abs/1901.10002">link</a></p>

  <p>Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. <em>arXiv:2011.13416.</em> Retrieved from <a href="https://arxiv.org/abs/2011.13416">link</a></p>

  <p><strong>Measurement and evaluation</strong> <br />
Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901</p>

  <p>Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256</p>

  <p><strong>Understanding and documenting limitations in ML</strong>   <br />
Alexander D’Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. Retrieved from https://arxiv.org/abs/2011.03395</p>

  <p>Jessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. Retrieved from https://arxiv.org/abs/2205.08363</p>

  <p>Margaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/3287560.3287596</p>

  <p>Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing Trust in AI Services through Supplier’s Declarations of Conformity. arXiv:1808.07261. Retrieved from https://arxiv.org/abs/1808.07261</p>

  <p>Michael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/3313831.3376445</p>

  <p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. Retrieved from https://arxiv.org/abs/1803.09010</p>

  <p>Bender, E. M., Friedman, B. &amp; McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. .https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf</p>

  <p><strong>Context of use</strong>   <br />
International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. Retrieved from https://www.iso.org/standard/77520.html</p>

  <p>National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. DOI: https://doi.org/10.6028/NIST.HB.161</p>

  <p><strong>Human-AI teaming</strong>  <br />
Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. DOI: https://doi.org/10.17226/26355.2022</p>

  <p>Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law &amp; Security Review 45 (26 Apr. 2021). DOI: https://dx.doi.org/10.2139/ssrn.3921216</p>

  <p>Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). Retrieved July 6, 2022 from https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html.</p>

  <p>Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI ‘21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. https://doi.org/10.1145/3411764.3445315</p>

  <p>Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). DOI: https://doi.org/10.1038/s41746-021-00385-9</p>

  <p>Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287</p>

  <p><strong>When not to deploy</strong>     <br />
Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* ‘20). Association for Computing Machinery, New York, NY, USA, 695. https://doi.org/10.1145/3351095.3375691</p>

  <p><strong>Statistical balance</strong>  <br />
Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342</p>

  <p><strong>Diversity</strong>     <br />
Sina Fazelpour and Maria De-Arteaga. 2022. Diversity in sociotechnical machine learning systems. Big Data &amp; Society 9, 1 (Jan. 2022). DOI: https://doi.org/10.1177%2F20539517221082027</p>

  <p><strong>Assessment of science in AI</strong>  <br />
Arvind Narayanan. How to recognize AI snake oil. Retrieved July 6, 2022 from https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf</p>

  <p>Emily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022). Retrieved July 6, 2022 from https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd</p>

</details>

			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="copyright" style="text-align:center;">&copy; AI RMF Playbook Released August 2022. </p>
	</div>
</footer>



 		<script src="/RMF/js/accordion.js"></script>

	</body>
</html>
