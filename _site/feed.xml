<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/RMF/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/RMF/" rel="alternate" type="text/html" /><updated>2022-08-05T12:21:58+00:00</updated><id>http://localhost:4000/RMF/</id><title type="html">AI RMF Playbook</title><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><entry><title type="html">MAP 5.3</title><link href="http://localhost:4000/RMF/map-5/2012/01/16/assess-impacts.html" rel="alternate" type="text/html" title="MAP 5.3" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-5/2012/01/16/assess-impacts</id><content type="html" xml:base="http://localhost:4000/RMF/map-5/2012/01/16/assess-impacts.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
The final output of the Map function is the go/no-go decision for deploying the AI system. This decision should take into account the risks mapped from previous steps and the organizational capacity for their management.&lt;/p&gt;

  &lt;p&gt;Risk mapping should also list system benefits beyond the status quo. Go/no-go decisions to deploy may be made by an independent third-party or organizational management. For higher risk systems, it is often appropriate – and may well be critical – for technical or risk executives to be involved in the approval of go/no-go decisions to deploy.&lt;/p&gt;

  &lt;p&gt;The decision to deploy should not be made by AI design and development teams, whose objective judgment may be hindered by the incentive to deploy.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Review and examine documentation, including system purpose and benefits, and mapped potential impacts with associated likelihoods.&lt;/li&gt;
    &lt;li&gt;Document the system’s estimated risk.&lt;/li&gt;
    &lt;li&gt;Make a go/no-go determination based on magnitude, and likelihood of impact. Do not deploy (no-go) or decommission the system if estimated risk surpasses organizational tolerances or thresholds. If a decision is made to proceed with deployment, assign the system to an appropriate risk tolerance and align oversight resources with the assessed risk.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 5.3&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;To what extent do these policies foster public trust and confidence in the use of the AI system?&lt;/li&gt;
    &lt;li&gt;What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?&lt;/li&gt;
    &lt;li&gt;How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 5.3&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Datasheets for Datasets&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;“AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019&lt;/li&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from &lt;a href=&quot;https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm&quot;&gt;Federal Reserve&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Elisa Jillson. 2021. Aiming for truth, fairness, and equity in your company’s use of AI (April 19, 2021). Retrieved on July 7, 2022 from &lt;a href=&quot;https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai&quot;&gt;FTC&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/2004.13676&quot;&gt;arXiv:2004.13676&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Sri Krishnamurthy. Quantifying Model Risk: Issues and approaches to measure and assess model risk when building quant models. QuantUniversity, Charlestown, MA. Retrieved on July 7, 2022 from &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.986.5412&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;citeseerx.ist.psu.edu&lt;/a&gt;&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? The final output of the Map function is the go/no-go decision for deploying the AI system. This decision should take into account the risks mapped from previous steps and the organizational capacity for their management. Risk mapping should also list system benefits beyond the status quo. Go/no-go decisions to deploy may be made by an independent third-party or organizational management. For higher risk systems, it is often appropriate – and may well be critical – for technical or risk executives to be involved in the approval of go/no-go decisions to deploy. The decision to deploy should not be made by AI design and development teams, whose objective judgment may be hindered by the incentive to deploy. How can organizations achieve the outcomes of this subcategory? Review and examine documentation, including system purpose and benefits, and mapped potential impacts with associated likelihoods. Document the system’s estimated risk. Make a go/no-go determination based on magnitude, and likelihood of impact. Do not deploy (no-go) or decommission the system if estimated risk surpasses organizational tolerances or thresholds. If a decision is made to proceed with deployment, assign the system to an appropriate risk tolerance and align oversight resources with the assessed risk. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 5.3 To what extent do these policies foster public trust and confidence in the use of the AI system? What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system? How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes? AI Transparency Resources: MAP 5.3 Datasheets for Datasets GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019 Intel.gov: AI Ethics Framework for Intelligence Community - 2020 Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019 What are some informative references? Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from Federal Reserve Elisa Jillson. 2021. Aiming for truth, fairness, and equity in your company’s use of AI (April 19, 2021). Retrieved on July 7, 2022 from FTC Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from arXiv:2004.13676 Sri Krishnamurthy. Quantifying Model Risk: Issues and approaches to measure and assess model risk when building quant models. QuantUniversity, Charlestown, MA. Retrieved on July 7, 2022 from citeseerx.ist.psu.edu</summary></entry><entry><title type="html">MAP 5.2</title><link href="http://localhost:4000/RMF/map-5/2012/01/16/likelihood-analysis.html" rel="alternate" type="text/html" title="MAP 5.2" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-5/2012/01/16/likelihood-analysis</id><content type="html" xml:base="http://localhost:4000/RMF/map-5/2012/01/16/likelihood-analysis.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
The likelihood of AI system impacts identified in Map 5.1 should be evaluated. Potential impacts should be documented and triaged.&lt;/p&gt;

  &lt;p&gt;Likelihood estimates may then be assessed and judged for go/no-go decisions about deploying an AI system. If an organization decides to proceed with deploying the system, the likelihood estimate can be used to assign oversight resources appropriate for the  risk level.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Establish assessment scales for measuring AI system impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly across the organization’s AI portfolio.&lt;/li&gt;
    &lt;li&gt;Apply impact assessments regularly at key stages in the AI lifecycle, connected to system impacts and frequency of system updates.&lt;/li&gt;
    &lt;li&gt;Assess system benefits and negative impacts in relation to trustworthy characteristics.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 5.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Which population(s) does the AI system impact?&lt;/li&gt;
    &lt;li&gt;What assessments has the entity conducted on data security and privacy impacts associated with the AI system?&lt;/li&gt;
    &lt;li&gt;Did you ensure that the AI system can be audited by independent third parties?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 5.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Datasheets for Datasets&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;“AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019&lt;/li&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from &lt;a href=&quot;https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en&quot;&gt;op.europa.eu&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Artificial Intelligence Incident Database. 2022. Retrieved from &lt;a href=&quot;https://incidentdatabase.ai/?lang=en&quot;&gt;Incidentdatabase&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? The likelihood of AI system impacts identified in Map 5.1 should be evaluated. Potential impacts should be documented and triaged. Likelihood estimates may then be assessed and judged for go/no-go decisions about deploying an AI system. If an organization decides to proceed with deploying the system, the likelihood estimate can be used to assign oversight resources appropriate for the risk level. How can organizations achieve the outcomes of this subcategory? Establish assessment scales for measuring AI system impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly across the organization’s AI portfolio. Apply impact assessments regularly at key stages in the AI lifecycle, connected to system impacts and frequency of system updates. Assess system benefits and negative impacts in relation to trustworthy characteristics. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 5.2 Which population(s) does the AI system impact? What assessments has the entity conducted on data security and privacy impacts associated with the AI system? Did you ensure that the AI system can be audited by independent third parties? AI Transparency Resources: MAP 5.2 Datasheets for Datasets GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019 Intel.gov: AI Ethics Framework for Intelligence Community - 2020 Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019 What are some informative references? Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from op.europa.eu Artificial Intelligence Incident Database. 2022. Retrieved from Incidentdatabase</summary></entry><entry><title type="html">MAP 5.1</title><link href="http://localhost:4000/RMF/map-5/2012/01/16/identify-impacts.html" rel="alternate" type="text/html" title="MAP 5.1" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-5/2012/01/16/identify-impacts</id><content type="html" xml:base="http://localhost:4000/RMF/map-5/2012/01/16/identify-impacts.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
AI systems are socio-technical in nature and can have positive, neutral, or negative implications that extend beyond their stated purpose. Negative impacts can be wide- ranging and affect individuals, groups, communities, organizations, and society, as well as the environment and national security.&lt;/p&gt;

  &lt;p&gt;The Map function provides an opportunity for organizations to assess potential AI system impacts based on identified risks. This enables organizations to create a baseline for system monitoring and to increase opportunities for detecting emergent risks. Impact assessments also help to identify new benefits and purposes which may arise from AI system use. After an AI system is deployed, engaging different stakeholder groups – who may be aware of, or experience, benefits or negative impacts that are unknown to AI actors – allows organizations to understand and monitor system benefits and impacts more readily.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Establish and document stakeholder engagement processes at the earliest stages of system formulation to identify potential impacts from the AI system on individuals, groups, communities, organizations, and society.&lt;/li&gt;
    &lt;li&gt;Employ methods such as value sensitive design (VSD) to identify misalignments between organizational and societal values, and system implementation and impact.&lt;/li&gt;
    &lt;li&gt;Identify approaches to engage, capture, and incorporate input from system users and other key stakeholders to assist with continuous monitoring for impacts and emergent risks. Incorporate quantitative, qualitative, and mixed methods in the assessment and documentation of potential impacts to individuals, groups, communities, organizations, and society.&lt;/li&gt;
    &lt;li&gt;Identify a team (internal or external) that is independent of AI design and development functions to assess AI system benefits, positive and negative impacts and their likelihood.&lt;/li&gt;
    &lt;li&gt;Develop impact assessment procedures that incorporate socio-technical elements and methods and plan to normalize across organizational culture. Regularly review and refine impact assessment processes.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 5.1&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?&lt;/li&gt;
    &lt;li&gt;If it relates to other ethically protected subjects, have appropriate obligations been met? (e.g., medical data might include information collected from animals)&lt;/li&gt;
    &lt;li&gt;If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial social or otherwise) What was done to mitigate or reduce the potential for harm?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 5.1&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Datasheets for Datasets&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;“AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019&lt;/li&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Susanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach for designing AI-based worker assistance systems in manufacturing. Procedia Comput. Sci. 200, C (2022), 505–516. &lt;a href=&quot;https://doi.org/10.1016/j.procs.2022.01.248&quot;&gt;DOI&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1901.10002&quot;&gt;arXiv:1901.10002&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from [arXiv:2011.13416()]https://arxiv.org/abs/2011.13416)&lt;/p&gt;

  &lt;p&gt;Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. &lt;a href=&quot;http://dx.doi.org/10.4236/jis.2013.41005&quot;&gt;DOI:&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Raji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., &amp;amp; Barnes, P. (2020). Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.&lt;/p&gt;

  &lt;p&gt;Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, &amp;amp; Jacob Metcalf. 2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest.  Data &amp;amp; Society. Accessed 7/14/2022 at &lt;a href=&quot;https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. &lt;a href=&quot;https://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare/&quot;&gt;adalovelaceinstitute&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Microsoft. Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. &lt;a href=&quot;https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf&quot;&gt;Microsoft-RAI-Impact-Assessment-Template&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Microsoft. Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. &lt;a href=&quot;https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf&quot;&gt;Microsoft-RAI-Impact-Assessment-Guide&lt;/a&gt;&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? AI systems are socio-technical in nature and can have positive, neutral, or negative implications that extend beyond their stated purpose. Negative impacts can be wide- ranging and affect individuals, groups, communities, organizations, and society, as well as the environment and national security. The Map function provides an opportunity for organizations to assess potential AI system impacts based on identified risks. This enables organizations to create a baseline for system monitoring and to increase opportunities for detecting emergent risks. Impact assessments also help to identify new benefits and purposes which may arise from AI system use. After an AI system is deployed, engaging different stakeholder groups – who may be aware of, or experience, benefits or negative impacts that are unknown to AI actors – allows organizations to understand and monitor system benefits and impacts more readily. How can organizations achieve the outcomes of this subcategory? Establish and document stakeholder engagement processes at the earliest stages of system formulation to identify potential impacts from the AI system on individuals, groups, communities, organizations, and society. Employ methods such as value sensitive design (VSD) to identify misalignments between organizational and societal values, and system implementation and impact. Identify approaches to engage, capture, and incorporate input from system users and other key stakeholders to assist with continuous monitoring for impacts and emergent risks. Incorporate quantitative, qualitative, and mixed methods in the assessment and documentation of potential impacts to individuals, groups, communities, organizations, and society. Identify a team (internal or external) that is independent of AI design and development functions to assess AI system benefits, positive and negative impacts and their likelihood. Develop impact assessment procedures that incorporate socio-technical elements and methods and plan to normalize across organizational culture. Regularly review and refine impact assessment processes. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 5.1 If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated? If it relates to other ethically protected subjects, have appropriate obligations been met? (e.g., medical data might include information collected from animals) If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial social or otherwise) What was done to mitigate or reduce the potential for harm? AI Transparency Resources: MAP 5.1 Datasheets for Datasets GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019 Intel.gov: AI Ethics Framework for Intelligence Community - 2020 Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019 What are some informative references? Susanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach for designing AI-based worker assistance systems in manufacturing. Procedia Comput. Sci. 200, C (2022), 505–516. DOI Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from arXiv:1901.10002 Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from [arXiv:2011.13416()]https://arxiv.org/abs/2011.13416) Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. DOI: Raji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., &amp;amp; Barnes, P. (2020). Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, &amp;amp; Jacob Metcalf. 2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest. Data &amp;amp; Society. Accessed 7/14/2022 at Link Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. adalovelaceinstitute Microsoft. Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. Microsoft-RAI-Impact-Assessment-Template Microsoft. Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. Microsoft-RAI-Impact-Assessment-Guide</summary></entry><entry><title type="html">MAP 4.2</title><link href="http://localhost:4000/RMF/map-4/2012/01/16/risk-controls-for-third-party-risks.html" rel="alternate" type="text/html" title="MAP 4.2" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-4/2012/01/16/risk-controls-for-third-party-risks</id><content type="html" xml:base="http://localhost:4000/RMF/map-4/2012/01/16/risk-controls-for-third-party-risks.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
In the course of their work, AI actors often utilize open-source, or otherwise freely available, third-party technologies – some of which have been reported to have privacy, bias, and security risks. Third-party entities providing AI technologies may not be subjected to the same procurement, human resource, or other risk controls which are applied to more standard technologies. Organizations may consider tightening up internal risk controls for these technology sources.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Supply resources such as model documentation templates and software safelists to assist in third-party technology inventory and approval activities.&lt;/li&gt;
    &lt;li&gt;Review third-party material (including data and models) for risks related to bias, data privacy, and security vulnerabilities.&lt;/li&gt;
    &lt;li&gt;Apply controls – such as procurement, security, and data privacy controls – to all acquired third-party technologies.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 4.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Did you ensure that the AI system can be audited by independent third parties?&lt;/li&gt;
    &lt;li&gt;To what extent do these policies foster public trust and confidence in the use of the AI system?&lt;/li&gt;
    &lt;li&gt;Did you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 4.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;WEF Model AI Governance Framework Assessment 2020&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Office of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from &lt;a href=&quot;https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html&quot;&gt;OCC&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;“Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021, available at &lt;a href=&quot;https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf&quot;&gt;URL&lt;/a&gt;&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? In the course of their work, AI actors often utilize open-source, or otherwise freely available, third-party technologies – some of which have been reported to have privacy, bias, and security risks. Third-party entities providing AI technologies may not be subjected to the same procurement, human resource, or other risk controls which are applied to more standard technologies. Organizations may consider tightening up internal risk controls for these technology sources. How can organizations achieve the outcomes of this subcategory? Supply resources such as model documentation templates and software safelists to assist in third-party technology inventory and approval activities. Review third-party material (including data and models) for risks related to bias, data privacy, and security vulnerabilities. Apply controls – such as procurement, security, and data privacy controls – to all acquired third-party technologies. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 4.2 Did you ensure that the AI system can be audited by independent third parties? To what extent do these policies foster public trust and confidence in the use of the AI system? Did you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)? AI Transparency Resources: MAP 4.2 GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities Intel.gov: AI Ethics Framework for Intelligence Community - 2020 WEF Model AI Governance Framework Assessment 2020 Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019 What are some informative references? Office of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from OCC “Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021, available at URL</summary></entry><entry><title type="html">MAP 4.1</title><link href="http://localhost:4000/RMF/map-4/2012/01/16/document-third-party-risks.html" rel="alternate" type="text/html" title="MAP 4.1" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-4/2012/01/16/document-third-party-risks</id><content type="html" xml:base="http://localhost:4000/RMF/map-4/2012/01/16/document-third-party-risks.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Due to the complex nature of AI technology, organizations often engage third-party support. Technologies and personnel from third-parties are another source of risk to consider during AI risk management activities. Such risks may be difficult to map since third-party provider risk tolerances may not be the same as the contracting institution.&lt;/p&gt;

  &lt;p&gt;Commercial, open source, and other freely-available technologies should be screened for third-party risks and transparently documented. For example, many reports suggest that the use of commercial large language models are a source of AI risks. These models are popular with AI developers due to their ease of use for inference tasks, and tend to rely on large uncurated web dataset or often have undisclosed origins. Large scale use of these models has raised concerns about privacy, bias, and unintended effects along with possible introduction of increased levels of statistical uncertainty, difficulty with reproducibility, and issues with scientific validity.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Review audit reports, testing results, product roadmaps, warranties, terms of service, end-user license agreements, contracts, and other documentation related to third-party entities to assist in value assessment and risk management activities.&lt;/li&gt;
    &lt;li&gt;Review third-party software release schedules and software change management plans (hotfixes, patches, updates, forward- and backward- compatibility guarantees) for irregularities that may contribute to AI system risks.&lt;/li&gt;
    &lt;li&gt;Inventory third-party material (hardware, open-source software, foundation models, open source data, proprietary software, proprietary data, etc.) required for system implementation and maintenance.&lt;/li&gt;
    &lt;li&gt;Review redundancies related to third-party technology and personnel to assess potential risks due to lack of adequate support.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 4.1&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Did you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?&lt;/li&gt;
    &lt;li&gt;If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?&lt;/li&gt;
    &lt;li&gt;How will the results independently verified?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 4.1&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;WEF Model AI Governance Framework Assessment 2020&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Language  models&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 610–623. &lt;a href=&quot;https://doi.org/10.1145/3442188.3445922&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Julia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics 10 (2022), 50–72.  &lt;a href=&quot;https://doi.org/10.1162/tacl_a_00447&quot;&gt;DOI:&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Laura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘22). Association for Computing Machinery, New York, NY, USA, 214–229. &lt;a href=&quot;https://doi.org/10.1145/3531146.3533088&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Office of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from &lt;a href=&quot;https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html&quot;&gt;OCC&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/2108.07258&quot;&gt;arXiv:2108.07258&lt;/a&gt;&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? Due to the complex nature of AI technology, organizations often engage third-party support. Technologies and personnel from third-parties are another source of risk to consider during AI risk management activities. Such risks may be difficult to map since third-party provider risk tolerances may not be the same as the contracting institution. Commercial, open source, and other freely-available technologies should be screened for third-party risks and transparently documented. For example, many reports suggest that the use of commercial large language models are a source of AI risks. These models are popular with AI developers due to their ease of use for inference tasks, and tend to rely on large uncurated web dataset or often have undisclosed origins. Large scale use of these models has raised concerns about privacy, bias, and unintended effects along with possible introduction of increased levels of statistical uncertainty, difficulty with reproducibility, and issues with scientific validity. How can organizations achieve the outcomes of this subcategory? Review audit reports, testing results, product roadmaps, warranties, terms of service, end-user license agreements, contracts, and other documentation related to third-party entities to assist in value assessment and risk management activities. Review third-party software release schedules and software change management plans (hotfixes, patches, updates, forward- and backward- compatibility guarantees) for irregularities that may contribute to AI system risks. Inventory third-party material (hardware, open-source software, foundation models, open source data, proprietary software, proprietary data, etc.) required for system implementation and maintenance. Review redundancies related to third-party technology and personnel to assess potential risks due to lack of adequate support. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 4.1 Did you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system? If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets? How will the results independently verified? AI Transparency Resources: MAP 4.1 GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities Intel.gov: AI Ethics Framework for Intelligence Community - 2020 WEF Model AI Governance Framework Assessment 2020 What are some informative references? Language models Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 610–623. Link Julia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics 10 (2022), 50–72. DOI: Laura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘22). Association for Computing Machinery, New York, NY, USA, 214–229. Link Office of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from OCC Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258. Retrieved from arXiv:2108.07258</summary></entry><entry><title type="html">MAP 3.3</title><link href="http://localhost:4000/RMF/map-3/2012/01/16/application-scope.html" rel="alternate" type="text/html" title="MAP 3.3" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-3/2012/01/16/application-scope</id><content type="html" xml:base="http://localhost:4000/RMF/map-3/2012/01/16/application-scope.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
After mapping context, the AI system scope should be narrowed. Systems that function in a narrow scope tend to enable better mapping, measurement, and management of risks in the learning or decision-making tasks and the system context. A narrow application scope also helps ease oversight functions and related resources within an organization.&lt;/p&gt;

  &lt;p&gt;For example, open-ended chatbot systems that interact with the public on the internet have a large number of risks that may be difficult to map, measure, and manage due to the variability from both the decision-making task and the operational context.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Consider narrowing contexts for system deployment, including factors related to:
      &lt;ul&gt;
        &lt;li&gt;How outcomes may directly or indirectly impact users and stakeholders.&lt;/li&gt;
        &lt;li&gt;Length of time the system is deployed in between re-trainings.&lt;/li&gt;
        &lt;li&gt;Geographical regions in which the system operates.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Engage AI actors in legal and procurement functions when specifying target application scope.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 3.3&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;To what extent has the entity clearly defined technical specifications and requirements for the AI system?&lt;/li&gt;
    &lt;li&gt;How do the technical specifications and requirements align with the AI system’s goals and objectives?&lt;/li&gt;
    &lt;li&gt;How might you respond to an intelligence consumer asking “How do you know this?”&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 3.3&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Mark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: Springer International Publishing, 2018.&lt;/p&gt;

  &lt;p&gt;Alice Zheng. 2015. Evaluating Machine Learning Models (2015). O’Reilly. Retrieved from https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/.&lt;/p&gt;

  &lt;p&gt;Brenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial intelligence. ABA Journal. Retrieved from https://www.abajournal.com/columns/article/5-things-lawyers-should-know-about-artificial-intelligence.&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">MAP 3.2</title><link href="http://localhost:4000/RMF/map-3/2012/01/16/system-cost.html" rel="alternate" type="text/html" title="MAP 3.2" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-3/2012/01/16/system-cost</id><content type="html" xml:base="http://localhost:4000/RMF/map-3/2012/01/16/system-cost.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Anticipating negative impacts of AI systems is a difficult task. Negative impacts can be due to many factors, such as poor system performance, and may range from minor annoyance to serious injury, financial losses, or regulatory enforcement actions. AI actors can work with a broad set of stakeholders to improve their capacity for assessing system impacts – and subsequently – system risks. Hasty or non-thorough impact assessments may result in erroneous determinations of no-risk for more complex or higher risk systems.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Perform a context analysis to map negative impacts arising from not integrating trustworthiness characteristics. When negative impacts are not direct or obvious, AI actors should engage with external stakeholders to investigate and document:
      &lt;ul&gt;
        &lt;li&gt;Who could be harmed?&lt;/li&gt;
        &lt;li&gt;What could be harmed?&lt;/li&gt;
        &lt;li&gt;When could harm arise?&lt;/li&gt;
        &lt;li&gt;How could harm arise?&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Implement procedures for regularly evaluating the qualitative and quantitative costs of internal and external AI system failures. Develop actions to prevent, detect, and/or correct  potential risks and related impacts. Regularly evaluate failure costs to inform go/no-go deployment decisions throughout the AI system lifecycle.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 3.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;To what extent does the system/entity consistently measure progress towards stated goals and objectives?&lt;/li&gt;
    &lt;li&gt;To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?&lt;/li&gt;
    &lt;li&gt;Have you documented and explained that machine errors may differ from human errors?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 3.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Abagayle Lee Blank. 2019. Computer vision machine learning and future-oriented ethics. Honors Project. Seattle Pacific University (SPU), Seattle, WA. Available at https://digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&amp;amp;context=honorsprojects&lt;/p&gt;

  &lt;p&gt;Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from https://arxiv.org/abs/2011.13416&lt;/p&gt;

  &lt;p&gt;Jeff Patton. 2014. User Story Mapping. O’Reilly, Sebastopol, CA. See https://www.jpattonassociates.com/story-mapping/&lt;/p&gt;

  &lt;p&gt;Margarita Boenig-Liptsin, Anissa Tanweer &amp;amp; Ari Edmundson (2022) Data Science Ethos Lifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2022.2089411&lt;/p&gt;

  &lt;p&gt;J. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, “The Four Pillars of Research Software Engineering,” in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, doi: 10.1109/MS.2020.2973362.&lt;/p&gt;

  &lt;p&gt;National Academies of Sciences, Engineering, and Medicine 2022. “Introduction” in Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. https://doi.org/10.17226/26507.&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">MAP 3.1</title><link href="http://localhost:4000/RMF/map-3/2012/01/16/system-benefits.html" rel="alternate" type="text/html" title="MAP 3.1" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-3/2012/01/16/system-benefits</id><content type="html" xml:base="http://localhost:4000/RMF/map-3/2012/01/16/system-benefits.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
AI system benefits should counterbalance the inherent risks and implicit and explicit costs. To identify system benefits, organizations should define and document system purpose and utility, along with foreseeable costs, risks, and negative impacts. Credible justification for anticipated benefits beyond the status quo should be clarified and documented.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Utilize participatory approaches and engage with system end users to evaluate system efficacy and interpretability of AI task output.&lt;/li&gt;
    &lt;li&gt;Incorporate stakeholder feedback about perceived system benefits beyond the status quo.&lt;/li&gt;
    &lt;li&gt;Align system requirements with intended purpose and document decisions.&lt;/li&gt;
    &lt;li&gt;Perform context analysis related to time frame, safety concerns, geographic area, physical environment, ecosystems, social environment, and cultural norms within the intended setting (or conditions that closely approximate the intended setting).&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 3.1&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Did you communicate the benefits of the AI system to users?&lt;/li&gt;
    &lt;li&gt;Did you provide appropriate training material and disclaimers to users on how to adequately use the AI system?&lt;/li&gt;
    &lt;li&gt;Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 3.1&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Intel.gov: AI Ethics Framework for Intelligence Community  - 2020&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555&lt;/p&gt;

  &lt;p&gt;Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 39–48. https://doi.org/10.1145/3287560.3287567&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">MAP 2.3</title><link href="http://localhost:4000/RMF/map-2/2012/01/16/data-collection-and-selection.html" rel="alternate" type="text/html" title="MAP 2.3" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-2/2012/01/16/data-collection-and-selection</id><content type="html" xml:base="http://localhost:4000/RMF/map-2/2012/01/16/data-collection-and-selection.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Many AI system risks can be traced to insufficient testing and evaluation processes. For example, machine learning requires large scale datasets. The difficulty of finding the “right” data may lead AI actors to select datasets based more on accessibility and availability than on suitability. Such decisions may contribute to an environment where the data used in processes is not fully representative of the populations or phenomena that are being modeled, inserting or introducing downstream risks.&lt;/p&gt;

  &lt;p&gt;Other risks arise when selected datasets and/or attributes within datasets are not good proxies, measures, or predictors for operationalizing the phenomenon that the AI system intends to support or inform. Practices such as dataset reuse may also lead to data becoming disconnected from the social contexts and time periods of their creation. Datasets may also present security concerns or be polluted by bad actors in an attempt to alter system outcomes.&lt;/p&gt;

  &lt;p&gt;Collected data may differ significantly from what occurs in the real world. Large scale datasets used in AI systems often do not include representation of people who have been historically excluded. This may have a disproportionately negative impact on black, indigenous, and people of color, women, LGBTQ+ individuals, people with disabilities, or people with limited access to computer network technologies.&lt;/p&gt;

  &lt;p&gt;To manage the dataset risks described above, it is important to:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;have a clear understanding of data content (e.g., data dictionaries, datasheets), data lineage, provenance, representativeness, legal basis for use, and security&lt;/li&gt;
    &lt;li&gt;follow experimental design best practices&lt;/li&gt;
    &lt;li&gt;consider data suitability and data privacy concerns&lt;/li&gt;
    &lt;li&gt;empirically validate underlying constructs and concepts in data selection.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Document assumptions made and techniques used during the selection, curation, preparation, and analysis of data, and when identifying constructs and proxy targets, and developing indices – especially when seeking to measure concepts that are inherently unobservable (e.g. “hireability,” “criminality.” “lendability”).&lt;/li&gt;
    &lt;li&gt;Map adherence to policies that address data and construct validity, bias, privacy and security for AI systems and verify documentation, oversight, and audit function processes.&lt;/li&gt;
    &lt;li&gt;Establish processes and practices that employ experimental design techniques for data collection, selection, and management practices.&lt;/li&gt;
    &lt;li&gt;Establish practices to ensure data used in AI systems is linked to the documented purpose of the AI system (e.g., by causal discovery methods).&lt;/li&gt;
    &lt;li&gt;Establish and document processes to ensure that test and training data lineage is well understood, traceable, and metadata resources are available for mapping risks.&lt;/li&gt;
    &lt;li&gt;Document known limitations, risk mitigation efforts associated with, and methods used for, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of missing, spurious, or outlier data; biased estimators).&lt;/li&gt;
    &lt;li&gt;Work with domain experts to:
      &lt;ul&gt;
        &lt;li&gt;Gain and maintain contextual awareness and knowledge about how human behavior is reflected in datasets, organizational factors and dynamics, and society.&lt;/li&gt;
        &lt;li&gt;Identify participatory approaches for responsible Human-AI configurations and oversight tasks, taking into account sources of cognitive bias.&lt;/li&gt;
        &lt;li&gt;Identify techniques to manage and mitigate sources of bias (systemic, computational, human-cognitive) in computational models and systems, and the assumptions and decisions in their development.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;Follow standard statistical principles and document the extent to which the proposed technology does not meet standard validation criteria..&lt;/li&gt;
    &lt;li&gt;Investigate and document potential negative impacts due to supply chain issues that may conflict with organizational values and principles.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 2.3&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Are there any known errors, sources of noise, or redundancies in the data?&lt;/li&gt;
    &lt;li&gt;Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame&lt;/li&gt;
    &lt;li&gt;What is the variable selection and evaluation process?&lt;/li&gt;
    &lt;li&gt;How was the data collected? Who was involved in the data collection process? If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)&lt;/li&gt;
    &lt;li&gt;As time passes and conditions change, is the training data still representative of the operational environment?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 2.3&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Datasheets for Datasets&lt;/li&gt;
    &lt;li&gt;WEF Model AI Governance Framework Assessment 2020&lt;/li&gt;
    &lt;li&gt;Companion to the Model AI Governance Framework- 2020&lt;/li&gt;
    &lt;li&gt;GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities&lt;/li&gt;
    &lt;li&gt;ATARC Model Transparency Assessment (WD) – 2020&lt;/li&gt;
    &lt;li&gt;Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Challenges with dataset selection&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). DOI: https://doi.org/10.3389/fdata.2019.00013&lt;/p&gt;

  &lt;p&gt;Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from https://arxiv.org/abs/2012.05345&lt;/p&gt;

  &lt;p&gt;Catherine D’Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, MA. See https://data-feminism.mitpress.mit.edu/&lt;/p&gt;

  &lt;p&gt;Miceli, M., &amp;amp; Posada, J. (2022). The Data-Production Dispositif. ArXiv, abs/2205.11963.&lt;/p&gt;

  &lt;p&gt;Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836. Retrieved from https://arxiv.org/abs/1608.07836&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Dataset and test, evaluation, validation and verification (TEVV) processes in AI system development&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;National Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. DOI: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf&lt;/p&gt;

  &lt;p&gt;Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the Everything in the Whole Wide World Benchmark. arXiv:2111.15366. Retrieved from https://arxiv.org/abs/2111.15366&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Statistical balance&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342&lt;/p&gt;

  &lt;p&gt;Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from https://arxiv.org/abs/2012.05345&lt;/p&gt;

  &lt;p&gt;Solon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, 368–378. https://doi.org/10.1145/3461702.3462610&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Measurement and evaluation&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901&lt;/p&gt;

  &lt;p&gt;Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Existing frameworks&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. URL: https://nvlpubs. nist. gov/nistpubs/CSWP/NIST. CSWP, 4162018.&lt;/p&gt;

  &lt;p&gt;Boeckl, K. R., &amp;amp; Lefkovitz, N. B. (2020). NIST privacy framework: A tool for improving privacy through enterprise risk management, version 1.0. URL:https://www.nist.gov/privacy-framework/privacy-framework&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? Many AI system risks can be traced to insufficient testing and evaluation processes. For example, machine learning requires large scale datasets. The difficulty of finding the “right” data may lead AI actors to select datasets based more on accessibility and availability than on suitability. Such decisions may contribute to an environment where the data used in processes is not fully representative of the populations or phenomena that are being modeled, inserting or introducing downstream risks. Other risks arise when selected datasets and/or attributes within datasets are not good proxies, measures, or predictors for operationalizing the phenomenon that the AI system intends to support or inform. Practices such as dataset reuse may also lead to data becoming disconnected from the social contexts and time periods of their creation. Datasets may also present security concerns or be polluted by bad actors in an attempt to alter system outcomes. Collected data may differ significantly from what occurs in the real world. Large scale datasets used in AI systems often do not include representation of people who have been historically excluded. This may have a disproportionately negative impact on black, indigenous, and people of color, women, LGBTQ+ individuals, people with disabilities, or people with limited access to computer network technologies. To manage the dataset risks described above, it is important to: have a clear understanding of data content (e.g., data dictionaries, datasheets), data lineage, provenance, representativeness, legal basis for use, and security follow experimental design best practices consider data suitability and data privacy concerns empirically validate underlying constructs and concepts in data selection. How can organizations achieve the outcomes of this subcategory? Document assumptions made and techniques used during the selection, curation, preparation, and analysis of data, and when identifying constructs and proxy targets, and developing indices – especially when seeking to measure concepts that are inherently unobservable (e.g. “hireability,” “criminality.” “lendability”). Map adherence to policies that address data and construct validity, bias, privacy and security for AI systems and verify documentation, oversight, and audit function processes. Establish processes and practices that employ experimental design techniques for data collection, selection, and management practices. Establish practices to ensure data used in AI systems is linked to the documented purpose of the AI system (e.g., by causal discovery methods). Establish and document processes to ensure that test and training data lineage is well understood, traceable, and metadata resources are available for mapping risks. Document known limitations, risk mitigation efforts associated with, and methods used for, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of missing, spurious, or outlier data; biased estimators). Work with domain experts to: Gain and maintain contextual awareness and knowledge about how human behavior is reflected in datasets, organizational factors and dynamics, and society. Identify participatory approaches for responsible Human-AI configurations and oversight tasks, taking into account sources of cognitive bias. Identify techniques to manage and mitigate sources of bias (systemic, computational, human-cognitive) in computational models and systems, and the assumptions and decisions in their development. Follow standard statistical principles and document the extent to which the proposed technology does not meet standard validation criteria.. Investigate and document potential negative impacts due to supply chain issues that may conflict with organizational values and principles. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 2.3 Are there any known errors, sources of noise, or redundancies in the data? Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame What is the variable selection and evaluation process? How was the data collected? Who was involved in the data collection process? If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.) As time passes and conditions change, is the training data still representative of the operational environment? AI Transparency Resources: MAP 2.3 Datasheets for Datasets WEF Model AI Governance Framework Assessment 2020 Companion to the Model AI Governance Framework- 2020 GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp;amp; Other Entities ATARC Model Transparency Assessment (WD) – 2020 Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020 What are some informative references? Challenges with dataset selection Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). DOI: https://doi.org/10.3389/fdata.2019.00013 Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from https://arxiv.org/abs/2012.05345 Catherine D’Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, MA. See https://data-feminism.mitpress.mit.edu/ Miceli, M., &amp;amp; Posada, J. (2022). The Data-Production Dispositif. ArXiv, abs/2205.11963. Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836. Retrieved from https://arxiv.org/abs/1608.07836 Dataset and test, evaluation, validation and verification (TEVV) processes in AI system development National Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. DOI: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the Everything in the Whole Wide World Benchmark. arXiv:2111.15366. Retrieved from https://arxiv.org/abs/2111.15366 Statistical balance Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342 Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from https://arxiv.org/abs/2012.05345 Solon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, 368–378. https://doi.org/10.1145/3461702.3462610 Measurement and evaluation Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901 Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256 Existing frameworks National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. URL: https://nvlpubs. nist. gov/nistpubs/CSWP/NIST. CSWP, 4162018. Boeckl, K. R., &amp;amp; Lefkovitz, N. B. (2020). NIST privacy framework: A tool for improving privacy through enterprise risk management, version 1.0. URL:https://www.nist.gov/privacy-framework/privacy-framework</summary></entry><entry><title type="html">MAP 2.2</title><link href="http://localhost:4000/RMF/map-2/2012/01/16/operational-context.html" rel="alternate" type="text/html" title="MAP 2.2" /><published>2012-01-16T00:00:00+00:00</published><updated>2012-01-16T00:00:00+00:00</updated><id>http://localhost:4000/RMF/map-2/2012/01/16/operational-context</id><content type="html" xml:base="http://localhost:4000/RMF/map-2/2012/01/16/operational-context.html">&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What is this subcategory about?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
Once deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors. One key weakness stems from developing systems in highly-controlled or optimized environments that differ considerably from the deployment context. This practice may contribute to an inability to anticipate downstream uses or constraints. AI actors can reduce the likelihood of such incidents through regular stakeholder engagement and feedback. This input can provide enhanced contextual awareness about how an AI system may interact in its real-world setting. Recommended practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;How can organizations achieve the outcomes of this subcategory?&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;Extend documentation beyond system and task requirements to include possible risks due to deployment contexts and human-AI configurations.&lt;/li&gt;
    &lt;li&gt;Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether users can correctly comprehend system outputs or results.&lt;/li&gt;
    &lt;li&gt;Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data.&lt;/li&gt;
    &lt;li&gt;Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment decisions.&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are the transparency and documentation considerations?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Transparency Considerations – Key Questions: MAP 2.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly?&lt;/li&gt;
    &lt;li&gt;To what extent is the output of each component appropriate for the operational context?&lt;/li&gt;
    &lt;li&gt;What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals     impacted by use of the AI system?&lt;/li&gt;
    &lt;li&gt;Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment)&lt;/li&gt;
    &lt;li&gt;How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;AI Transparency Resources: MAP 2.2&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Datasheets for Datasets&lt;/li&gt;
    &lt;li&gt;WEF Model AI Governance Framework Assessment 2020&lt;/li&gt;
    &lt;li&gt;Companion to the Model AI Governance Framework- 2020&lt;/li&gt;
    &lt;li&gt;ATARC Model Transparency Assessment (WD) – 2020&lt;/li&gt;
    &lt;li&gt;Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;What are some informative references?&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Context of use&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. Retrieved from https://www.iso.org/standard/77520.html&lt;/p&gt;

  &lt;p&gt;National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. DOI: https://doi.org/10.6028/NIST.HB.161&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Human-machine interaction&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Smith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.&lt;/p&gt;

  &lt;p&gt;Warden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100&lt;/p&gt;

  &lt;p&gt;Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. DOI: https://doi.org/10.17226/26355.2022&lt;/p&gt;

  &lt;p&gt;Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law &amp;amp; Security Review 45 (26 Apr. 2021). DOI: https://dx.doi.org/10.2139/ssrn.3921216&lt;/p&gt;

  &lt;p&gt;Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). Retrieved July 6, 2022 from https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html.&lt;/p&gt;

  &lt;p&gt;Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI ‘21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. https://doi.org/10.1145/3411764.3445315&lt;/p&gt;

  &lt;p&gt;Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). DOI: https://doi.org/10.1038/s41746-021-00385-9&lt;/p&gt;

  &lt;p&gt;Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287&lt;/p&gt;

&lt;/details&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">What is this subcategory about? Once deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors. One key weakness stems from developing systems in highly-controlled or optimized environments that differ considerably from the deployment context. This practice may contribute to an inability to anticipate downstream uses or constraints. AI actors can reduce the likelihood of such incidents through regular stakeholder engagement and feedback. This input can provide enhanced contextual awareness about how an AI system may interact in its real-world setting. Recommended practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions. How can organizations achieve the outcomes of this subcategory? Extend documentation beyond system and task requirements to include possible risks due to deployment contexts and human-AI configurations. Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether users can correctly comprehend system outputs or results. Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data. Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment decisions. What are the transparency and documentation considerations? Transparency Considerations – Key Questions: MAP 2.2 Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly? To what extent is the output of each component appropriate for the operational context? What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system? Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment) How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI? AI Transparency Resources: MAP 2.2 Datasheets for Datasets WEF Model AI Governance Framework Assessment 2020 Companion to the Model AI Governance Framework- 2020 ATARC Model Transparency Assessment (WD) – 2020 Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020 What are some informative references? Context of use International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. Retrieved from https://www.iso.org/standard/77520.html National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. DOI: https://doi.org/10.6028/NIST.HB.161 Human-machine interaction Smith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515. Warden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100 Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. DOI: https://doi.org/10.17226/26355.2022 Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law &amp;amp; Security Review 45 (26 Apr. 2021). DOI: https://dx.doi.org/10.2139/ssrn.3921216 Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). Retrieved July 6, 2022 from https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html. Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI ‘21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. https://doi.org/10.1145/3411764.3445315 Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). DOI: https://doi.org/10.1038/s41746-021-00385-9 Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287</summary></entry></feed>