<!DOCTYPE html>
<link rel="stylesheet" href="https://pages.nist.gov/nist-header-footer/css/nist-combined.css">
      <script src="https://pages.nist.gov/nist-header-footer/js/jquery-1.9.0.min.js" type="text/javascript" defer="defer"></script>
      <script src="https://pages.nist.gov/nist-header-footer/js/nist-header-footer.js" type="text/javascript" defer="defer"></script>


<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="/RMF/css/screen.css">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">

		

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Search | AI RMF Playbook</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/RMF/search.html" />
<meta property="og:url" content="http://localhost:4000/RMF/search.html" />
<meta property="og:site_name" content="AI RMF Playbook" />
<script type="application/ld+json">
{"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/RMF/NISTlogo.png"}},"headline":"Search","@type":"WebPage","url":"http://localhost:4000/RMF/search.html","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/RMF/feed.xml" title="AI RMF Playbook" />

	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
                                        <nav>
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/" >HOME</a>
                         </button>
                    
                
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/govern/" >GOVERN</a>
                         </button>
                    
                
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/map/" >MAP</a>
                         </button>
                    
                
	
	
		
                
                     <a class="">MEASURE</a>
                
	
	
		
                
                     <a class="">MANAGE</a>
                
	
	
		
                
                    
                    	 <button class="navbtn scale">
		    	 <a href="/RMF/terms/" >TERMS</a>
                         </button>
                    
                
	
	
		
                
                    
       			 <button  class="navbtn scale"><a class="">&nbsp;</a></button>
        		 <button  class="navbtn scale"><a href="/RMF/about/">ABOUT</a></button>
                    
                
	
</nav>


				</section>
				<section class="hero_search">
					<h1>AI Risk Managment Framework Playbook</h1>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					
					"map-5-2012-01-16-assess-impacts-html": {
						"id": "map-5-2012-01-16-assess-impacts-html",
						"title": "MAP 5.3",
						"categories": "MAP-5",
						"url": " /map-5/2012/01/16/assess-impacts.html",
						"content": "What is this subcategory about?\n  \nThe final output of the Map function is the go/no-go decision for deploying the AI system. This decision should take into account the risks mapped from previous steps and the organizational capacity for their management.\n\n  Risk mapping should also list system benefits beyond the status quo. Go/no-go decisions to deploy may be made by an independent third-party or organizational management. For higher risk systems, it is often appropriate – and may well be critical – for technical or risk executives to be involved in the approval of go/no-go decisions to deploy.\n\n  The decision to deploy should not be made by AI design and development teams, whose objective judgment may be hindered by the incentive to deploy.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Review and examine documentation, including system purpose and benefits, and mapped potential impacts with associated likelihoods.\n    Document the system’s estimated risk.\n    Make a go/no-go determination based on magnitude, and likelihood of impact. Do not deploy (no-go) or decommission the system if estimated risk surpasses organizational tolerances or thresholds. If a decision is made to proceed with deployment, assign the system to an appropriate risk tolerance and align oversight resources with the assessed risk.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 5.3\n  \n    To what extent do these policies foster public trust and confidence in the use of the AI system?\n    What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n    How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n  \n\n  AI Transparency Resources: MAP 5.3\n  \n    Datasheets for Datasets\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019\n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n  \n\n\n\n\n  What are some informative references?\n  \nBoard of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from Federal Reserve\n\n  Elisa Jillson. 2021. Aiming for truth, fairness, and equity in your company’s use of AI (April 19, 2021). Retrieved on July 7, 2022 from FTC\n\n  Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from arXiv:2004.13676\n\n  Sri Krishnamurthy. Quantifying Model Risk: Issues and approaches to measure and assess model risk when building quant models. QuantUniversity, Charlestown, MA. Retrieved on July 7, 2022 from citeseerx.ist.psu.edu"
					}
					
				
			
		
			
				
					,
					
					"map-5-2012-01-16-likelihood-analysis-html": {
						"id": "map-5-2012-01-16-likelihood-analysis-html",
						"title": "MAP 5.2",
						"categories": "MAP-5",
						"url": " /map-5/2012/01/16/likelihood-analysis.html",
						"content": "What is this subcategory about?\n  \nThe likelihood of AI system impacts identified in Map 5.1 should be evaluated. Potential impacts should be documented and triaged.\n\n  Likelihood estimates may then be assessed and judged for go/no-go decisions about deploying an AI system. If an organization decides to proceed with deploying the system, the likelihood estimate can be used to assign oversight resources appropriate for the  risk level.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Establish assessment scales for measuring AI system impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly across the organization’s AI portfolio.\n    Apply impact assessments regularly at key stages in the AI lifecycle, connected to system impacts and frequency of system updates.\n    Assess system benefits and negative impacts in relation to trustworthy characteristics.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 5.2\n  \n    Which population(s) does the AI system impact?\n    What assessments has the entity conducted on data security and privacy impacts associated with the AI system?\n    Did you ensure that the AI system can be audited by independent third parties?\n  \n\n  AI Transparency Resources: MAP 5.2\n  \n    Datasheets for Datasets\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019\n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n  \n\n\n\n\n  What are some informative references?\n  \nEmilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from op.europa.eu\n\n  Artificial Intelligence Incident Database. 2022. Retrieved from Incidentdatabase"
					}
					
				
			
		
			
				
					,
					
					"map-5-2012-01-16-identify-impacts-html": {
						"id": "map-5-2012-01-16-identify-impacts-html",
						"title": "MAP 5.1",
						"categories": "MAP-5",
						"url": " /map-5/2012/01/16/identify-impacts.html",
						"content": "What is this subcategory about?\n  \nAI systems are socio-technical in nature and can have positive, neutral, or negative implications that extend beyond their stated purpose. Negative impacts can be wide- ranging and affect individuals, groups, communities, organizations, and society, as well as the environment and national security.\n\n  The Map function provides an opportunity for organizations to assess potential AI system impacts based on identified risks. This enables organizations to create a baseline for system monitoring and to increase opportunities for detecting emergent risks. Impact assessments also help to identify new benefits and purposes which may arise from AI system use. After an AI system is deployed, engaging different stakeholder groups – who may be aware of, or experience, benefits or negative impacts that are unknown to AI actors – allows organizations to understand and monitor system benefits and impacts more readily.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Establish and document stakeholder engagement processes at the earliest stages of system formulation to identify potential impacts from the AI system on individuals, groups, communities, organizations, and society.\n    Employ methods such as value sensitive design (VSD) to identify misalignments between organizational and societal values, and system implementation and impact.\n    Identify approaches to engage, capture, and incorporate input from system users and other key stakeholders to assist with continuous monitoring for impacts and emergent risks. Incorporate quantitative, qualitative, and mixed methods in the assessment and documentation of potential impacts to individuals, groups, communities, organizations, and society.\n    Identify a team (internal or external) that is independent of AI design and development functions to assess AI system benefits, positive and negative impacts and their likelihood.\n    Develop impact assessment procedures that incorporate socio-technical elements and methods and plan to normalize across organizational culture. Regularly review and refine impact assessment processes.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 5.1\n  \n    If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?\n    If it relates to other ethically protected subjects, have appropriate obligations been met? (e.g., medical data might include information collected from animals)\n    If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial social or otherwise) What was done to mitigate or reduce the potential for harm?\n  \n\n  AI Transparency Resources: MAP 5.1\n  \n    Datasheets for Datasets\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019\n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n  \n\n\n\n\n  What are some informative references?\n  \nSusanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach for designing AI-based worker assistance systems in manufacturing. Procedia Comput. Sci. 200, C (2022), 505–516. DOI\n\n  Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from arXiv:1901.10002\n\n  Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from [arXiv:2011.13416()]https://arxiv.org/abs/2011.13416)\n\n  Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. DOI:\n\n  Raji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., &amp; Barnes, P. (2020). Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.\n\n  Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, &amp; Jacob Metcalf. 2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest.  Data &amp; Society. Accessed 7/14/2022 at Link\n\n  Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. adalovelaceinstitute\n\n  Microsoft. Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. Microsoft-RAI-Impact-Assessment-Template\n\n  Microsoft. Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. Microsoft-RAI-Impact-Assessment-Guide"
					}
					
				
			
		
			
				
					,
					
					"map-4-2012-01-16-risk-controls-for-third-party-risks-html": {
						"id": "map-4-2012-01-16-risk-controls-for-third-party-risks-html",
						"title": "MAP 4.2",
						"categories": "MAP-4",
						"url": " /map-4/2012/01/16/risk-controls-for-third-party-risks.html",
						"content": "What is this subcategory about?\n  \nIn the course of their work, AI actors often utilize open-source, or otherwise freely available, third-party technologies – some of which have been reported to have privacy, bias, and security risks. Third-party entities providing AI technologies may not be subjected to the same procurement, human resource, or other risk controls which are applied to more standard technologies. Organizations may consider tightening up internal risk controls for these technology sources.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Supply resources such as model documentation templates and software safelists to assist in third-party technology inventory and approval activities.\n    Review third-party material (including data and models) for risks related to bias, data privacy, and security vulnerabilities.\n    Apply controls – such as procurement, security, and data privacy controls – to all acquired third-party technologies.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 4.2\n  \n    Did you ensure that the AI system can be audited by independent third parties?\n    To what extent do these policies foster public trust and confidence in the use of the AI system?\n    Did you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?\n  \n\n  AI Transparency Resources: MAP 4.2\n  \n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    WEF Model AI Governance Framework Assessment 2020\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n  \n\n\n\n\n  What are some informative references?\n  \nOffice of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from OCC\n\n  “Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021, available at URL"
					}
					
				
			
		
			
				
					,
					
					"map-4-2012-01-16-document-third-party-risks-html": {
						"id": "map-4-2012-01-16-document-third-party-risks-html",
						"title": "MAP 4.1",
						"categories": "MAP-4",
						"url": " /map-4/2012/01/16/document-third-party-risks.html",
						"content": "What is this subcategory about?\n  \nDue to the complex nature of AI technology, organizations often engage third-party support. Technologies and personnel from third-parties are another source of risk to consider during AI risk management activities. Such risks may be difficult to map since third-party provider risk tolerances may not be the same as the contracting institution.\n\n  Commercial, open source, and other freely-available technologies should be screened for third-party risks and transparently documented. For example, many reports suggest that the use of commercial large language models are a source of AI risks. These models are popular with AI developers due to their ease of use for inference tasks, and tend to rely on large uncurated web dataset or often have undisclosed origins. Large scale use of these models has raised concerns about privacy, bias, and unintended effects along with possible introduction of increased levels of statistical uncertainty, difficulty with reproducibility, and issues with scientific validity.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Review audit reports, testing results, product roadmaps, warranties, terms of service, end-user license agreements, contracts, and other documentation related to third-party entities to assist in value assessment and risk management activities.\n    Review third-party software release schedules and software change management plans (hotfixes, patches, updates, forward- and backward- compatibility guarantees) for irregularities that may contribute to AI system risks.\n    Inventory third-party material (hardware, open-source software, foundation models, open source data, proprietary software, proprietary data, etc.) required for system implementation and maintenance.\n    Review redundancies related to third-party technology and personnel to assess potential risks due to lack of adequate support.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 4.1\n  \n    Did you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n    If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?\n    How will the results independently verified?\n  \n\n  AI Transparency Resources: MAP 4.1\n  \n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    WEF Model AI Governance Framework Assessment 2020\n  \n\n\n\n\n  What are some informative references?\n  \nLanguage  models\n\n  Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 610–623. Link\n\n  Julia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics 10 (2022), 50–72.  DOI:\n\n  Laura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘22). Association for Computing Machinery, New York, NY, USA, 214–229. Link\n\n  Office of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from OCC\n\n  Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258. Retrieved from arXiv:2108.07258"
					}
					
				
			
		
			
				
					,
					
					"map-3-2012-01-16-application-scope-html": {
						"id": "map-3-2012-01-16-application-scope-html",
						"title": "MAP 3.3",
						"categories": "MAP-3",
						"url": " /map-3/2012/01/16/application-scope.html",
						"content": "What is this subcategory about?\n  \nAfter mapping context, the AI system scope should be narrowed. Systems that function in a narrow scope tend to enable better mapping, measurement, and management of risks in the learning or decision-making tasks and the system context. A narrow application scope also helps ease oversight functions and related resources within an organization.\n\n  For example, open-ended chatbot systems that interact with the public on the internet have a large number of risks that may be difficult to map, measure, and manage due to the variability from both the decision-making task and the operational context.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Consider narrowing contexts for system deployment, including factors related to:\n      \n        How outcomes may directly or indirectly impact users and stakeholders.\n        Length of time the system is deployed in between re-trainings.\n        Geographical regions in which the system operates.\n      \n    \n    Engage AI actors in legal and procurement functions when specifying target application scope.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 3.3\n  \n    To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n    How do the technical specifications and requirements align with the AI system’s goals and objectives?\n    How might you respond to an intelligence consumer asking “How do you know this?”\n  \n\n  AI Transparency Resources: MAP 3.3\n  \n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n  \n\n\n\n\n  What are some informative references?\n  \nMark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: Springer International Publishing, 2018.\n\n  Alice Zheng. 2015. Evaluating Machine Learning Models (2015). O’Reilly. Retrieved from https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/.\n\n  Brenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial intelligence. ABA Journal. Retrieved from https://www.abajournal.com/columns/article/5-things-lawyers-should-know-about-artificial-intelligence."
					}
					
				
			
		
			
				
					,
					
					"map-3-2012-01-16-system-cost-html": {
						"id": "map-3-2012-01-16-system-cost-html",
						"title": "MAP 3.2",
						"categories": "MAP-3",
						"url": " /map-3/2012/01/16/system-cost.html",
						"content": "What is this subcategory about?\n  \nAnticipating negative impacts of AI systems is a difficult task. Negative impacts can be due to many factors, such as poor system performance, and may range from minor annoyance to serious injury, financial losses, or regulatory enforcement actions. AI actors can work with a broad set of stakeholders to improve their capacity for assessing system impacts – and subsequently – system risks. Hasty or non-thorough impact assessments may result in erroneous determinations of no-risk for more complex or higher risk systems.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Perform a context analysis to map negative impacts arising from not integrating trustworthiness characteristics. When negative impacts are not direct or obvious, AI actors should engage with external stakeholders to investigate and document:\n      \n        Who could be harmed?\n        What could be harmed?\n        When could harm arise?\n        How could harm arise?\n      \n    \n    Implement procedures for regularly evaluating the qualitative and quantitative costs of internal and external AI system failures. Develop actions to prevent, detect, and/or correct  potential risks and related impacts. Regularly evaluate failure costs to inform go/no-go deployment decisions throughout the AI system lifecycle.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 3.2\n  \n    To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n    To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n    Have you documented and explained that machine errors may differ from human errors?\n  \n\n  AI Transparency Resources: MAP 3.2\n  \n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n  \n\n\n\n\n  What are some informative references?\n  \nAbagayle Lee Blank. 2019. Computer vision machine learning and future-oriented ethics. Honors Project. Seattle Pacific University (SPU), Seattle, WA. Available at https://digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&amp;context=honorsprojects\n\n  Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from https://arxiv.org/abs/2011.13416\n\n  Jeff Patton. 2014. User Story Mapping. O’Reilly, Sebastopol, CA. See https://www.jpattonassociates.com/story-mapping/\n\n  Margarita Boenig-Liptsin, Anissa Tanweer &amp; Ari Edmundson (2022) Data Science Ethos Lifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2022.2089411\n\n  J. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, “The Four Pillars of Research Software Engineering,” in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, doi: 10.1109/MS.2020.2973362.\n\n  National Academies of Sciences, Engineering, and Medicine 2022. “Introduction” in Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. https://doi.org/10.17226/26507."
					}
					
				
			
		
			
				
					,
					
					"map-3-2012-01-16-system-benefits-html": {
						"id": "map-3-2012-01-16-system-benefits-html",
						"title": "MAP 3.1",
						"categories": "MAP-3",
						"url": " /map-3/2012/01/16/system-benefits.html",
						"content": "What is this subcategory about?\n  \nAI system benefits should counterbalance the inherent risks and implicit and explicit costs. To identify system benefits, organizations should define and document system purpose and utility, along with foreseeable costs, risks, and negative impacts. Credible justification for anticipated benefits beyond the status quo should be clarified and documented.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Utilize participatory approaches and engage with system end users to evaluate system efficacy and interpretability of AI task output.\n    Incorporate stakeholder feedback about perceived system benefits beyond the status quo.\n    Align system requirements with intended purpose and document decisions.\n    Perform context analysis related to time frame, safety concerns, geographic area, physical environment, ecosystems, social environment, and cultural norms within the intended setting (or conditions that closely approximate the intended setting).\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 3.1\n  \n    Did you communicate the benefits of the AI system to users?\n    Did you provide appropriate training material and disclaimers to users on how to adequately use the AI system?\n    Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n  \n\n  AI Transparency Resources: MAP 3.1\n  \n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n  \n\n\n\n\n  What are some informative references?\n  \nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555\n\n  Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 39–48. https://doi.org/10.1145/3287560.3287567"
					}
					
				
			
		
			
				
					,
					
					"map-2-2012-01-16-data-collection-and-selection-html": {
						"id": "map-2-2012-01-16-data-collection-and-selection-html",
						"title": "MAP 2.3",
						"categories": "MAP-2",
						"url": " /map-2/2012/01/16/data-collection-and-selection.html",
						"content": "What is this subcategory about?\n  \nMany AI system risks can be traced to insufficient testing and evaluation processes. For example, machine learning requires large scale datasets. The difficulty of finding the “right” data may lead AI actors to select datasets based more on accessibility and availability than on suitability. Such decisions may contribute to an environment where the data used in processes is not fully representative of the populations or phenomena that are being modeled, inserting or introducing downstream risks.\n\n  Other risks arise when selected datasets and/or attributes within datasets are not good proxies, measures, or predictors for operationalizing the phenomenon that the AI system intends to support or inform. Practices such as dataset reuse may also lead to data becoming disconnected from the social contexts and time periods of their creation. Datasets may also present security concerns or be polluted by bad actors in an attempt to alter system outcomes.\n\n  Collected data may differ significantly from what occurs in the real world. Large scale datasets used in AI systems often do not include representation of people who have been historically excluded. This may have a disproportionately negative impact on black, indigenous, and people of color, women, LGBTQ+ individuals, people with disabilities, or people with limited access to computer network technologies.\n\n  To manage the dataset risks described above, it is important to:\n  \n    have a clear understanding of data content (e.g., data dictionaries, datasheets), data lineage, provenance, representativeness, legal basis for use, and security\n    follow experimental design best practices\n    consider data suitability and data privacy concerns\n    empirically validate underlying constructs and concepts in data selection.\n  \n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Document assumptions made and techniques used during the selection, curation, preparation, and analysis of data, and when identifying constructs and proxy targets, and developing indices – especially when seeking to measure concepts that are inherently unobservable (e.g. “hireability,” “criminality.” “lendability”).\n    Map adherence to policies that address data and construct validity, bias, privacy and security for AI systems and verify documentation, oversight, and audit function processes.\n    Establish processes and practices that employ experimental design techniques for data collection, selection, and management practices.\n    Establish practices to ensure data used in AI systems is linked to the documented purpose of the AI system (e.g., by causal discovery methods).\n    Establish and document processes to ensure that test and training data lineage is well understood, traceable, and metadata resources are available for mapping risks.\n    Document known limitations, risk mitigation efforts associated with, and methods used for, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of missing, spurious, or outlier data; biased estimators).\n    Work with domain experts to:\n      \n        Gain and maintain contextual awareness and knowledge about how human behavior is reflected in datasets, organizational factors and dynamics, and society.\n        Identify participatory approaches for responsible Human-AI configurations and oversight tasks, taking into account sources of cognitive bias.\n        Identify techniques to manage and mitigate sources of bias (systemic, computational, human-cognitive) in computational models and systems, and the assumptions and decisions in their development.\n      \n    \n    Follow standard statistical principles and document the extent to which the proposed technology does not meet standard validation criteria..\n    Investigate and document potential negative impacts due to supply chain issues that may conflict with organizational values and principles.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 2.3\n  \n    Are there any known errors, sources of noise, or redundancies in the data?\n    Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame\n    What is the variable selection and evaluation process?\n    How was the data collected? Who was involved in the data collection process? If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)\n    As time passes and conditions change, is the training data still representative of the operational environment?\n  \n\n  AI Transparency Resources: MAP 2.3\n  \n    Datasheets for Datasets\n    WEF Model AI Governance Framework Assessment 2020\n    Companion to the Model AI Governance Framework- 2020\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    ATARC Model Transparency Assessment (WD) – 2020\n    Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020\n  \n\n\n\n\n  What are some informative references?\n  \nChallenges with dataset selection\n\n  Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). DOI: https://doi.org/10.3389/fdata.2019.00013\n\n  Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from https://arxiv.org/abs/2012.05345\n\n  Catherine D’Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, MA. See https://data-feminism.mitpress.mit.edu/\n\n  Miceli, M., &amp; Posada, J. (2022). The Data-Production Dispositif. ArXiv, abs/2205.11963.\n\n  Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836. Retrieved from https://arxiv.org/abs/1608.07836\n\n  Dataset and test, evaluation, validation and verification (TEVV) processes in AI system development\n\n  National Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. DOI: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf\n\n  Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the Everything in the Whole Wide World Benchmark. arXiv:2111.15366. Retrieved from https://arxiv.org/abs/2111.15366\n\n  Statistical balance\n\n  Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342\n\n  Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from https://arxiv.org/abs/2012.05345\n\n  Solon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, 368–378. https://doi.org/10.1145/3461702.3462610\n\n  Measurement and evaluation\n\n  Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901\n\n  Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256\n\n  Existing frameworks\n\n  National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. URL: https://nvlpubs. nist. gov/nistpubs/CSWP/NIST. CSWP, 4162018.\n\n  Boeckl, K. R., &amp; Lefkovitz, N. B. (2020). NIST privacy framework: A tool for improving privacy through enterprise risk management, version 1.0. URL:https://www.nist.gov/privacy-framework/privacy-framework"
					}
					
				
			
		
			
				
					,
					
					"map-2-2012-01-16-operational-context-html": {
						"id": "map-2-2012-01-16-operational-context-html",
						"title": "MAP 2.2",
						"categories": "MAP-2",
						"url": " /map-2/2012/01/16/operational-context.html",
						"content": "What is this subcategory about?\n  \nOnce deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors. One key weakness stems from developing systems in highly-controlled or optimized environments that differ considerably from the deployment context. This practice may contribute to an inability to anticipate downstream uses or constraints. AI actors can reduce the likelihood of such incidents through regular stakeholder engagement and feedback. This input can provide enhanced contextual awareness about how an AI system may interact in its real-world setting. Recommended practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Extend documentation beyond system and task requirements to include possible risks due to deployment contexts and human-AI configurations.\n    Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether users can correctly comprehend system outputs or results.\n    Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data.\n    Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment decisions.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 2.2\n  \n    Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly?\n    To what extent is the output of each component appropriate for the operational context?\n    What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals     impacted by use of the AI system?\n    Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment)\n    How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?\n  \n\n  AI Transparency Resources: MAP 2.2\n  \n    Datasheets for Datasets\n    WEF Model AI Governance Framework Assessment 2020\n    Companion to the Model AI Governance Framework- 2020\n    ATARC Model Transparency Assessment (WD) – 2020\n    Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020\n  \n\n\n\n\n  What are some informative references?\n  \nContext of use\n\n  International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. Retrieved from https://www.iso.org/standard/77520.html\n\n  National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. DOI: https://doi.org/10.6028/NIST.HB.161\n\n  Human-machine interaction\n\n  Smith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.\n\n  Warden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100\n\n  Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. DOI: https://doi.org/10.17226/26355.2022\n\n  Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law &amp; Security Review 45 (26 Apr. 2021). DOI: https://dx.doi.org/10.2139/ssrn.3921216\n\n  Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). Retrieved July 6, 2022 from https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html.\n\n  Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI ‘21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. https://doi.org/10.1145/3411764.3445315\n\n  Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). DOI: https://doi.org/10.1038/s41746-021-00385-9\n\n  Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287"
					}
					
				
			
		
			
				
					,
					
					"map-2-2012-01-16-learning-task-html": {
						"id": "map-2-2012-01-16-learning-task-html",
						"title": "MAP 2.1",
						"categories": "MAP-2",
						"url": " /map-2/2012/01/16/learning-task.html",
						"content": "What is this subcategory about?\n  \nAI actors should define the technical learning or decision-making task an AI system is designed to accomplish, along with the benefits that the system will provide. The clearer and narrower the task definition, the easier it is to map its benefits and risks, leading to more fulsome risk management.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Narrowly define and document AI system learning task(s) along with known assumptions and limitations.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 2.1\n\n  \n    To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n    To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?\n    How do the technical specifications and requirements align with the AI system’s goals and objectives?\n    Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?\n    How are outputs marked to clearly show that they came from an AI?\n  \n\n  AI Transparency Resources: MAP 2.1\n\n  \n    Datasheets for Datasets\n    WEF Model AI Governance Framework Assessment 2020\n    Companion to the Model AI Governance Framework- 2020\n    ATARC Model Transparency Assessment (WD) – 2020\n    Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020\n  \n\n\n\n\n  What are some informative references?\n  \nLeong, Brenda (2020). The Spectrum of Artificial Intelligence - An Infographic Tool. Future of Privacy Forum. Retrieved from https://fpf.org/blog/the-spectrum-of-artificial-intelligence-an-infographic-tool/\n\n  Brownlee, Jason (2020). A Tour of Machine Learning Algorithms. Machine Learning Mastery. Retrieved from https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/."
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-system-requirements-html": {
						"id": "map-1-2012-01-16-system-requirements-html",
						"title": "MAP 1.7",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/system-requirements.html",
						"content": "What is this subcategory about?\n  \nAI system development requirements may outpace documentation processes for traditional software. When written requirements are unavailable or incomplete, AI actors may inadvertently overlook business and stakeholder needs, or over-rely on implicit human biases such as confirmation bias and groupthink. To mitigate the influence of these implicit factors, AI actors can seek input from, and develop transparent and actionable recourse mechanisms for, end-users and operators. Engaging external stakeholders in this process  integrates broader perspectives on socio-technical risk factors. Incorporating trustworthy characteristics early in the design phase should be a priority – instead of forcing a solution onto existing systems.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Proactively incorporate trustworthy characteristics into system requirements.\n    Consider risk factors related to Human-AI configurations and tasks.\n    Analyze dependencies between contextual factors and system requirements. List impacts that may arise from not fully considering the importance of  trustworthiness characteristics in any decision making.\n    Follow responsible design techniques in tasks such as software engineering, product management, and participatory engagement. Some examples for eliciting and documenting stakeholder requirements include product requirement documents (PRDs), user stories, user interaction/user experience (UI/UX) research, systems engineering, ethnography and related field methods.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 1.7\n  \n    What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n    To what extent is this information sufficient and appropriate to promote transparency? Promote transparency by enabling external stakeholders to access information on the design, operation, and limitations of the AI system.\n    To what extent has relevant information been disclosed regarding the use of AI systems, such as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) what its limitations are? (Documentation and external communication can offer a way for entities to provide transparency.)\n    What metrics has the entity developed to measure performance of the AI system?\n    What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?\n  \n\n  AI Transparency Resources: MAP 1.7\n  \n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    “Stakeholders in Explainable AI,” Sep. 2018, [Online]. Available: http://arxiv.org/abs/1810.00184\n    “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n    “HIGH-LEVEL EXPERT GROUP ON ARTIFICIAL INTELLIGENCE SET UP BY THE EUROPEAN COMMISSION ETHICS GUIDELINES FOR TRUSTWORTHY AI.” [Online]. Available: https://ec.europa.eu/digital-\n  \n\n\n\n\n  What are some informative references?\n  \nNational Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. https://doi.org/10.17226/26507.\n\n  Amit K. Chopra, Fabiano Dalpiaz, F. Başak Aydemir, et al. 2014. Protos: Foundations for engineering innovative sociotechnical systems. In 2014 IEEE 22nd International Requirements Engineering Conference (RE) (2014), 53-62. DOI: https://doi.org/10.1109/RE.2014.6912247\n\n  Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 59–68. https://doi.org/10.1145/3287560.3287598\n\n  Gordon Baxter and Ian Sommerville. 2011. Socio-technical systems: From design methods to systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4–17. DOI: https://doi.org/10.1016/j.intcom.2010.07.003\n\n  Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555\n\n  Yilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-96550-5_6"
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-stakeholders-html": {
						"id": "map-1-2012-01-16-stakeholders-html",
						"title": "MAP 1.6",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/stakeholders.html",
						"content": "What is this subcategory about?\n  \nRisk management should include processes for regular and meaningful communication with stakeholder groups. Stakeholders can provide valuable input related to system gaps and limitations. Organizations may differ in the types and number of stakeholders with which they engage.\n\n  Participatory approaches such as human-centered design (HCD) and value-sensitive design (VSD) can help AI teams to engage broadly with stakeholder communities. This type of engagement can enable AI teams to learn about how a given technology may cause impacts, both positive and negative, that were not originally considered or intended.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Maintain awareness and documentation of the individuals, groups, or communities who make up the system’s internal and external stakeholders.\n    Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to system design or deployment decisions.\n    Verify that appropriate skills and practices are available in-house for carrying out stakeholder engagement activities such as eliciting, capturing, and synthesizing stakeholder feedback, and translating it for AI design and development functions.\n    Define which AI actors, beyond AI design and development teams, will review system design, implementation, and operation tasks. Define which AI actors will administer and implement test, evaluation, verification, and validation (TEVV) tasks across the AI lifecycle.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 1.6\n  \n    To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n    Who is responsible for checking the AI at these intervals?\n    What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n    If anyone believed that the AI no longer meets this ethical framework, who will be responsible for receiving the concern and as appropriate investigating and remediating the issue? Do they have authority to modify, limit, or stop the use of the AI?\n    How easily accessible and current is the information available to external stakeholders?\n  \n\n  AI Transparency Resources: MAP 1.6\n  \n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    “Stakeholders in Explainable AI,” Sep. 2018, [Online]. Available: http://arxiv.org/abs/1810.00184\n    “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n    “HIGH-LEVEL EXPERT GROUP ON ARTIFICIAL INTELLIGENCE SET UP BY THE EUROPEAN COMMISSION ETHICS GUIDELINES FOR TRUSTWORTHY AI.” [Online]. Available: https://ec.europa.eu/digital-\n  \n\n\n\n\n  What are some informative references?\n  \nVincent T. Covello. 2021. Stakeholder Engagement and Empowerment. In Communicating in Risk, Crisis, and High Stress Situations (Vincent T. Covello, ed.), 87-109. DOI: https://doi.org/10.1002/9781119081753.ch5\n\n  Yilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-96550-5_6\n\n  Eloise Taysom and Nathan Crilly. 2017. Resilience in Sociotechnical Systems: The Perspectives of Multiple Stakeholders. She Ji: The Journal of Design, Economics, and Innovation, 3, 3 (2017), 165-182, ISSN 2405-8726. DOI: https://doi.org/10.1016/j.sheji.2017.10.011"
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-risk-tolerance-html": {
						"id": "map-1-2012-01-16-risk-tolerance-html",
						"title": "MAP 1.5",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/risk-tolerance.html",
						"content": "What is this subcategory about?\n  \nRisk tolerance reflects the level and type of risk the organization will accept while conducting its mission and carrying out its strategy.\n\n  Deployment should not be pre-determined. Rather, it should result from a clearly defined process based on organizational risk tolerances.\n\n  Go/no-go decisions should be incorporated throughout the AI system’s lifecycle. For systems deemed “higher risk,” such decisions should include approval from relevant technical or risk-focused executives.\n\n  Go/no-go decisions related to AI system risks should take stakeholder feedback into account, but remain independent from stakeholders’ vested financial or reputational interests\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Establish risk tolerance levels for AI systems and allocate the appropriate oversight resources to each level.\n    Identify maximum allowable risk thresholds above which the system will not be deployed, within the contextual or application setting. Do not deploy AI systems that exceed organizational risk tolerances.\n    Attempts to use a system for “off-label” purposes should be approached with caution, especially in settings that organizations have deemed as high-risk. Document decisions, risk-related trade-offs, and system limitations.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 1.5\n  \n    What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?\n    How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n    To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n  \n\n  AI Transparency Resources: MAP 1.5\n  \n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    WEF Model AI Governance Framework Assessment 2020\n    Companion to the WEF Model AI Governance Framework- 2020\n  \n\n\n\n\n  What are some informative references?\n  \nBoard of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm\n\n  The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Retrieved on July 12, 2022 from https://www.occ.treas.gov/publications-and-resources/publications/banker-education/files/pub-risk-appetite-statement.pdf"
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-mission-goals-html": {
						"id": "map-1-2012-01-16-mission-goals-html",
						"title": "MAP 1.4",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/mission-goals.html",
						"content": "What is this subcategory about?\n  \nSocio-technical AI risks emerge from the interplay between technical development decisions and how a system is used, who operates it, and the social context into which it is deployed. Establishing a process for clear and comprehensive enumeration of AI system purpose and expectations can assist organizations in identifying and managing risks.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Reconcile documented concerns about system context of use or purpose against the organization’s stated values, mission statements, social responsibility commitments, and AI principles.\n    Reconsider the design, implementation strategy, or deployment of AI systems with potential impacts that do not reflect institutional values.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nTransparency Considerations – Key Questions: MAP 1.4\n  \n    What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?\n    To what extent are the model outputs consistent with the entity’s values and principles to foster public trust and equity?\n    To what extent are the metrics consistent with system goals, objectives, and constraints, including ethical and compliance considerations?\n  \n\n  AI Transparency Resources: MAP 1.4\n  \n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n    WEF Model AI Governance Framework Assessment 2020\n    “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n  \n\n\n\n\n  What are some informative references?\n  \nAlgorithm Watch. AI Ethics Guidelines Global Inventory. Retrieved from https://inventory.algorithmwatch.org/\n\n  Emanuel Moss and Jacob Metcalf. 2020. Ethics Owners: A New Model of Organizational Responsibility in Data-Driven Technology Companies. Data &amp; Society Research Institute. Retrieved from https://datasociety.net/pubs/Ethics-Owners.pdf\n\n  Future of Life Institute. Asilomar AI Principles. Retrieved from https://futureoflife.org/2017/08/11/ai-principles/\n\n  Leonard Haas, Sebastian Gießler, and Veronika Thiel. 2020. In the realm of paper tigers – exploring the failings of AI ethics guidelines. (April 28, 2020). Retrieved on July 6, 2022 from https://algorithmwatch.org/en/ai-ethics-guidelines-inventory-upgrade-2020/"
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-business-value-html": {
						"id": "map-1-2012-01-16-business-value-html",
						"title": "MAP 1.3",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/business-value.html",
						"content": "What is this subcategory about?\n  \nAI systems should present a business benefit beyond the status quo when considering inherent risks and implicit or explicit costs. Defining and documenting the specific business purpose of an AI system helps teams to evaluate risks and increases the clarity of “go/no-go” decisions about whether to deploy.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Build transparent practices into AI system development processes.\n    Review the documented system purpose from a socio-technical perspective and in consideration of societal values.\n    Determine possible misalignment between societal values and stated organizational principles and code of ethics.\n    Flag latent incentives that may contribute to negative impacts.\n    Balance AI system purpose with potential risks, societal values, and stated organizational principles.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n\n  Transparency Considerations – Key Questions: MAP 1.3\n  \n    How does the AI system help the entity meet its goals and objectives?\n    How do the technical specifications and requirements align with the AI system’s goals and objectives?\n    To what extent is the output of each component appropriate for the operational context?\n    What (other) tasks could the dataset be used for? Are there obvious tasks for which it should not be used?\n  \n\n  AI Transparency Resources: MAP 1.3\n  \n    Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n    Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n    Datasheets for Datasets”\n  \n\n\n\n\n  What are some informative references?\n  \nAbeba Birhane, Pratyusha Kalluri, Dallas Card, et al. 2022. The Values Encoded in Machine Learning Research. arXiv:2106.15590. Retrieved from https://arxiv.org/abs/2106.15590\n\n  Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm"
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-ai-actors-html": {
						"id": "map-1-2012-01-16-ai-actors-html",
						"title": "MAP 1.2",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/ai-actors.html",
						"content": "What is this subcategory about?\n  \nSuccessfully mapping context requires a team of AI actors with a diversity of experience, expertise, abilities and backgrounds, and with the resources and independence to engage in critical inquiry. A diverse and empowered team increases the ability of an organization to broaden their contextual perspectives, check their assumptions about context of use, recognize when systems are not functional within and outside of the intended context, and identify constraints in real world applications that may lead to harmful impacts.\n\n  Having a diverse team contributes to more open sharing of ideas and assumptions about the purpose and function of the technology being designed and developed – making these implicit aspects more explicit. The benefit of a diverse staff in managing AI risks is not the beliefs or presumed beliefs of individual workers, but the behavior that results from a collective perspective. An environment which fosters critical inquiry creates opportunities to surface problems and identify existing and emergent risks.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    \n      Establish interdisciplinary teams to reflect a wide range of skills, competencies, and capacity for AI efforts. Verify that team membership includes both demographic diversity, broad domain expertise, and lived experiences. Document team composition.\n    \n    \n      Empower interdisciplinary teams to capture, learn, and engage the interdependencies of deployed AI systems and related terminologies and concepts from disciplines outside of AI practice such as law, sociology, psychology, anthropology, public policy, systems design, and engineering.\n    \n  \n\n\n\n\n  What are the transparency and documentation considerations?\n\n  Column H content goes here.\n\n\n\n\n  What are some informative references?\n  \nSina Fazelpour and Maria De-Arteaga. 2022. Diversity in sociotechnical machine learning systems. Big Data &amp; Society 9, 1 (Jan. 2022). DOI: https://doi.org/10.1177%2F20539517221082027"
					}
					
				
			
		
			
				
					,
					
					"map-1-2012-01-16-intended-purpose-html": {
						"id": "map-1-2012-01-16-intended-purpose-html",
						"title": "MAP 1.1",
						"categories": "MAP-1",
						"url": " /map-1/2012/01/16/intended-purpose.html",
						"content": "What is this subcategory about?\n  \nMapping context may include examination of the following:\n  \n    intended and actual deployment setting.\n    specific set of users.\n    operator or subject expectations.\n    concept of operations.\n    intended purpose and impact of system use.\n    requirements for system deployment and operation.\n    potential negative impacts to individuals, groups, communities, organizations, and society – or context-specific impacts such as legal requirements or impacts to the environment.\n    unintended, downstream, or other unknown contextual factors.\n  \n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n\n  \n    Pursue AI system design purposefully, after non-AI solutions are considered.\n    Collaboratively consider intended AI system design tasks along with unanticipated purposes.\n    Maintain awareness of industry, technical, and applicable legal standards.\n    Track and document existing AI systems held by the organization, and those maintained or supported by third-party entities.\n    Gain and maintain awareness about evaluating scientific claims related to AI system performance and benefits before launching into system design.\n    Define and document the task, purpose, minimum functionality, and benefits of the AI system to inform considerations about whether the project is worth pursuing.\n    Define the AI system context of use, including:\n      \n        operational environment\n        impacts to individuals, groups, communities, organizations, and society\n        user characteristics and tasks\n        social environment.\n      \n    \n    Determine the user and organizational requirements, including business and technical requirements.\n    Identify human-AI interaction and/or roles, such as whether the application will support or replace human decision making.\n    Plan for risks related to human-AI configurations, and document requirements, roles, and responsibilities for human oversight of deployed systems.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n\n  Transparency Considerations – Key Questions: MAP 1.1\n  \n    Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?\n    Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n    Who is accountable for the ethical considerations during all stages of the AI lifecycle?\n    Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap that needed to be filled?\n    How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?\n  \n\n  AI Transparency Resources: MAP 1.1\n  \n    Datasheets for Datasets\n    GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n    “Stakeholders in Explainable AI,” Sep. 2018, [Online]. link\n  \n\n\n\n\n  What are some informative references?\n  \nSocio-technical systems\n\n  Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 59–68. https://doi.org/10.1145/3287560.3287598\n\n  Problem formulation\n\n  Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555\n\n  Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 39–48. https://doi.org/10.1145/3287560.3287567\n\n  Context mapping\n\n  Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en\n\n  Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from https://arxiv.org/abs/2004.13676\n\n  Social Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change Projects (Draft v2.0). Retrieved from https://www.alnap.org/system/files/content/resource/files/main/Draft%20SIMLab%20Context%20Analysis%20Framework%20v2.0.pdf\n\n  Solon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. Commun. ACM 64, 7 (July 2021), 30–32. https://doi.org/10.1145/3466612\n\n  Identification of harms\n\n  Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from https://arxiv.org/abs/1901.10002\n\n  Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from https://arxiv.org/abs/2011.13416\n\n  Measurement and evaluation\n\n  Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901\n\n  Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256\n\n  Understanding and documenting limitations in ML\n\n  Alexander D’Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. Retrieved from https://arxiv.org/abs/2011.03395\n\n  Jessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. Retrieved from https://arxiv.org/abs/2205.08363\n\n  Margaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/3287560.3287596\n\n  Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing Trust in AI Services through Supplier’s Declarations of Conformity. arXiv:1808.07261. Retrieved from https://arxiv.org/abs/1808.07261\n\n  Michael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/3313831.3376445\n\n  Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. Retrieved from https://arxiv.org/abs/1803.09010\n\n  Bender, E. M., Friedman, B. &amp; McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. .https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf\n\n  When not to deploy\n\n  Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* ‘20). Association for Computing Machinery, New York, NY, USA, 695. https://doi.org/10.1145/3351095.3375691\n\n  Statistical balance\n\n  Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342\n\n  Assessment of science in AI\n\n  Arvind Narayanan. How to recognize AI snake oil. Retrieved July 6, 2022 from https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf\n\n  Emily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022). Retrieved July 6, 2022 from https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd"
					}
					
				
			
		
			
				
					,
					
					"govern-6-2012-01-16-congingencies-for-third-party-systems-html": {
						"id": "govern-6-2012-01-16-congingencies-for-third-party-systems-html",
						"title": "GOVERN 6.2",
						"categories": "GOVERN-6",
						"url": " /govern-6/2012/01/16/congingencies-for-third-party-systems.html",
						"content": "What is this subcategory about?\n  \nTo mitigate the potential harms of third-party system failure, organizations should implement policies and procedures that include redundancies for covering third-party functions.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish policies for handling third-party system failures to include consideration of redundancy mechanisms for vital third-party AI systems.\n    Ensure that incident response plans address third-party AI systems.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n  “Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021, available at https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf\n\n  Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html"
					}
					
				
			
		
			
				
					,
					
					"govern-6-2012-01-16-guideline-for-third-party-systems-html": {
						"id": "govern-6-2012-01-16-guideline-for-third-party-systems-html",
						"title": "GOVERN 6.1",
						"categories": "GOVERN-6",
						"url": " /govern-6/2012/01/16/guideline-for-third-party-systems.html",
						"content": "What is this subcategory about?\n  \nOrganizations usually engage multiple third parties for external expertise, data, software packages (both open source and commercial), and software and hardware platforms across the AI lifecycle.\n\n  The need to rely on external resources or expertise may heighten existing challenges in an already complex undertaking, increasing the difficulty of risk management efforts.\n\n  Organizational approaches to managing third-party risk should be tailored to the resources, risk profile, and use case for each system. Organizations should strive to apply governance approaches to third-party AI system and datal as they would for internal resources — including open source software, publicly available data, and commercially available models.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n  \n    Collaboratively establish policies that address third-party AI systems and data.\n    Establish policies related to:\n      \n        Transparency into third-party system functions, including knowledge about training data, training and inference algorithms, and assumptions and limitations.\n        Thorough testing of third-party AI systems.\n        Requirements for clear and complete instructions for third-party system usage.\n      \n    \n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n  “Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021, available at https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf\n\n  Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html"
					}
					
				
			
		
			
				
					,
					
					"govern-5-2012-01-16-post-feedback-html": {
						"id": "govern-5-2012-01-16-post-feedback-html",
						"title": "GOVERN 5.2",
						"categories": "GOVERN-5",
						"url": " /govern-5/2012/01/16/post-feedback.html",
						"content": "What is this subcategory about?\n  \nRisk tolerance reflects the level and type of risk the organization will accept while conducting its mission and carrying out its strategy. Organizational leaders establish risk tolerances that others in the organization can follow via more fine-grained policies and procedures.\n\n  Organizations should accept that AI-related incidents can and will occur, focus on risk mitigation, and emphasize practical detection and mitigation.\n\n  When risks arise, resources are allocated based on the assessed risk of a given AI system. Organizations should apply a risk tolerance approach in which higher risk systems receive larger allocations of risk management resources and lower risk systems receive less resources. Such allocation schemes are necessary to target limited risk management resources. Acknowledgement of risks also implies that some systems may be too risky to deploy given the organization’s risk tolerance, and organizational policies should address such scenarios.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Explicitly acknowledge that AI systems, and the use of AI, present inherent costs and risks along with potential benefits.\n    Define reasonable risk tolerances for AI systems informed by laws, regulation, best practices, or industry standards.\n    Establish policies that define how to assign AI systems to established risk tolerance levels by combining system impact assessments with the likelihood that an impact occurs. Such assessment often entails some combination of:\n      \n        Econometric evaluations of impacts and impact likelihoods to assess AI system risk.\n        Red-amber-green (RAG) scales for impact severity and likelihood to assess AI system risk.\n        Establishment of policies for allocating risk management resources along established risk tolerance levels, with higher-risk systems receive more risk management resources and oversight.\n        Establishment of policies for approval, conditional approval, and disapproval of the design, implementation, and deployment of AI systems.\n      \n    \n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n  Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html\n\n  The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Retrieved on July 12, 2022 from https://www.occ.treas.gov/publications-and-resources/publications/banker-education/files/pub-risk-appetite-statement.pdf"
					}
					
				
			
		
			
				
					,
					
					"govern-5-2012-01-16-capturing-stakeholder-feedback-html": {
						"id": "govern-5-2012-01-16-capturing-stakeholder-feedback-html",
						"title": "GOVERN 5.1",
						"categories": "GOVERN-5",
						"url": " /govern-5/2012/01/16/capturing-stakeholder-feedback.html",
						"content": "What is this subcategory about?\n  \nInternal and laboratory-based system testing is crucially important, but the true test of an AI system is whether it is fit for purpose in a real-world setting, its risk is managed and its negative impact is minimized.\n\n  Participatory stakeholder engagement is one type of qualitative activity to help AI actor teams answer questions such as whether to pursue a project or how to design with impact in mind. The consideration of how to convene a group and the kinds of individuals, groups, or community organizations to include is an iterative process connected to the purpose of the system being pursued. Other factors relate to how to collaboratively and respectfully capture stakeholder feedback and insight that is useful, without being a solely perfunctory exercise.\n\n  These activities are best carried out by personnel with expertise in participatory practices, qualitative methods, and translation of contextual feedback for technical audiences.\n\n  Participatory engagement is not a one-time exercise and should be carried out from the very beginning of AI system commissioning through the end of the lifecycle. Organizations can consider how to incorporate engagement when beginning a project and as part of their monitoring of systems. Engagement is often utilized as a consultative practice, but this perspective may inadvertently lead to “participation washing.”  Organizational transparency about the purpose and goal of the engagement can help mitigate that possibility.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Ensure AI risk management policies address explicit mechanisms for receiving, processing, and implementing stakeholder and user feedback that could include:\n      \n        Recourse mechanisms for faulty AI system outputs.\n        Bug bounties.\n        Human-centered design.\n        User-interaction and experience research.\n        Participatory stakeholder engagement with individuals and communities that may experience negative impacts.\n      \n    \n    Ensure that stakeholder feedback is considered and addressed, including environmental concerns, and across the entire population of intended users, including historically excluded populations, people with disabilities, older people, and those with limited access to the internet and other basic technologies.\n    Clarify the organization’s principles as they apply to AI systems – considering those which have been proposed publicly – to inform external stakeholders of the organization’s values. Consider publishing or adopting AI principles.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nISO, “Ergonomics of human-system interaction — Part 210: Human-centered design for interactive systems,” ISO 9241-210:2019 (2nd ed.), July 2019, https://www.iso.org/standard/77520.html.\n\n  Rumman Chowdhury and Jutta Williams, “Introducing Twitter’s first algorithmic bias bounty challenge,” https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge.\n\n  Leonard Haas and Sebastian Gießler, “In the realm of paper tigers – exploring the failings of AI ethics guidelines,” AlgorithmWatch, 2020, available at https://algorithmwatch.org/en/ai-ethics-guidelines-inventory-upgrade-2020/.\n\n  Josh Kenway, Camille Francois, Dr. Sasha Costanza-Chock, Inioluwa Deborah Raji, &amp; Dr. Joy Buolamwini. 2022. Bug Bounties for Algorithmic Harms? Algorithmic Justice League. Accessed July 14, 2022. https://www.ajl.org/bugs"
					}
					
				
			
		
			
				
					,
					
					"govern-4-2012-01-16-practices-html": {
						"id": "govern-4-2012-01-16-practices-html",
						"title": "GOVERN 4.3",
						"categories": "GOVERN-4",
						"url": " /govern-4/2012/01/16/practices.html",
						"content": "What is this subcategory about?\n  \nOrganizations committed to risk management acknowledge the importance of identifying AI system limitations, detecting and tracking negative impacts and incidents, and sharing information about these issues with appropriate AI actors. Building organizational capacity requires policies and procedures connected to testing and inquiry.\n\n  Issues such as concept drift, AI bias and discrimination, shortcut learning or underspecification are difficult to identify using standard AI testing processes. Organizations can institute in-house use and testing policies and procedures to identify and manage such issues. Efforts can take the form of pre-alpha or pre-beta testing, or deploying internally developed systems or products within the organization. Testing may entail limited and controlled in-house, or publicly available, AI system testbeds.\n\n  Without policies and procedures that enable consistent testing practices, risk management efforts may be bypassed or ignored, exacerbating risks or leading to inconsistent risk management activities.\n\n  Information sharing about impacts or incidents detected during testing or deployment can:\n  \n    draw attention to AI system risks, failures, abuses and misuses,\n    allow organizations to benefit from insights based on a wide range of AI applications and implementations, and\n    allow organizations to be more proactive in avoiding known failure modes.\n  \n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish policies and procedures to facilitate and equip AI system testing.\n    Establish organizational commitment to identifying AI system limitations and sharing of insights about limitations within appropriate AI actor groups.\n    Establish guidelines for handling and access control related to AI system risks and performance.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nS. McGregor, “Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,” arXiv:2011.08512 [cs], Nov. 2020, arXiv:2011.08512. [Online]. Available: http://arxiv.org/abs/2011.08512\n\n  C. Johnson, M. Badger, D. Waltermire, J. Snyder, and C. Skorupka, “Guide to cyber threat information sharing,” National Institute of Standards and Technology, NIST Special Publication 800-150, Nov 2016. [Online]. Available: https://doi.org/10.6028/NIST.SP.800-150"
					}
					
				
			
		
			
				
					,
					
					"govern-4-2012-01-16-team-empowerment-html": {
						"id": "govern-4-2012-01-16-team-empowerment-html",
						"title": "GOVERN 4.2",
						"categories": "GOVERN-4",
						"url": " /govern-4/2012/01/16/team-empowerment.html",
						"content": "What is this subcategory about?\n  \nImpact assessments are a recent approach for driving responsible and ethical technology development practices. And, within a specific use case, these assessments can provide a high-level structure for organizations to frame risks of a given algorithm or deployment. Impact assessments can also serve as a mechanism for organizations to articulate risks and generate documentation for mitigation and oversight activities when harms do arise.\n\n  Impact assessments should be applied at the beginning of a process but also iteratively and regularly since goals and outcomes can evolve over time. It is also important to consider conflicts of interest, or undue influence, related to the organizational team being assessed.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish impact assessment policies and processes for AI systems used by the organization.\n    Ensure impact assessment policies are appropriate to evaluate the potential negative impact of a system and how quickly a system changes, and that assessments are applied on a regular basis.\n    Ensure impact assessments are utilized to inform broader assessments of AI system risk.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n  Patrick Hall, Navdeep Gill, and Benjamin Cox, “Responsible Machine Learning,” O’Reilly Media, 2020, available at https://www.oreilly.com/library/view/responsible-machine-learning/9781492090878/\n\n  Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking\nInstitutions, E-23 (Sept. 2017).\n\n  GAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,” GAO@100 (GAO-21-519SP), June 2021, https://www.gao.gov/assets/gao-21-519sp.pdf.\n\n  Donald Sull, Stefano Turconi, and Charles Sull, “When It Comes to Culture, Does Your Company Walk the Talk?” MIT Sloan Mgmt. Rev., 2020, https://sloanreview.mit.edu/article/when-it-comes-to-culture-does-your-company-walk-the-talk."
					}
					
				
			
		
			
				
					,
					
					"govern-4-2012-01-16-teams-and-communications-html": {
						"id": "govern-4-2012-01-16-teams-and-communications-html",
						"title": "GOVERN 4.1",
						"categories": "GOVERN-4",
						"url": " /govern-4/2012/01/16/teams-and-communications.html",
						"content": "What is this subcategory about?\n  \nA strong risk culture and accompanying practices can help organizations effectively triage the most critical risks. Organizations in some industries implement three (or more) “lines of defense,” where separate teams are held accountable for different aspects of the system lifecycle, such as development, risk management, and auditing.  While a traditional three-lines approach may be impractical for smaller organizations, leadership can commit to cultivating a strong risk culture through other means. For example, “effective challenge,” is a culture-based practice that encourages critical thinking and questioning of important design and implementation decisions by experts with the authority and stature to make such changes.\n\n  Red-teaming is also another risk management approach. This practice consists of adversarial testing of AI systems under stress conditions to seek out failure modes or vulnerabilities in the system. Red-teams are composed of external experts or personnel who are independent from the AI design and development teams.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish policies that require oversight functions (legal, compliance, risk management) from the outset of the system design process.\n    Establish policies that promote effective challenge of AI system design, implementation, and deployment decisions, via mechanisms such as the three lines of defense, model audits, or red-teaming – to ensure that workplace risks such as groupthink do not take hold.\n    Establish policies that incentivize general critical thinking and review at an organizational and procedural level.\n    Establish whistleblower protections for insiders who report on perceived serious problems with AI systems.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nH.R. 2231, 116th Cong. (2019), https://www.congress.gov/bill/116th-congress/house-bill/2231/text\n\n  BSA The Software Alliance (2021) Confronting Bias: BSA’s Framework to Build Trust in AI. https://www.bsa.org/reports/confronting-bias-bsas-framework-to-build-trust-in-ai\n\n  David Wright, “Making Privacy Impact Assessments More Effective.” The Information Society 29, 2013, available at https://iapp.org/media/pdf/knowledge_center/Making_PIA__more_effective.pdf.\n\n  E. Moss, E. Watkins, R. Singh, M. Elish, and J. Metcalf, “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” [Online]. Available: https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/\n\n  M. Kop, “AI Impact Assessment &amp; Code of Conduct,” Futurium, May 2019, https://futurium.ec.europa.eu/en/european-ai-alliance/best-practices/ai-impact-assessment-code-conduct.\n\n  D. Reisman, J. Schultz, K. Crawford, and M. Whittaker, “Algorithmic Impact Assessments: A Practical Framework For Public Agency Accountability,” AI Now, Apr. 2018, https://ainowinstitute.org/aiareport2018.pdf.\n\n  A. D. Selbst, “An Institutional View Of Algorithmic Impact Assessments,” Harvard Journal of Law &amp; Technology, vol. 35, no. 1, 2021\n\n  Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. https://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare/"
					}
					
				
			
		
			
				
					,
					
					"govern-3-2012-01-16-team-composition-html": {
						"id": "govern-3-2012-01-16-team-composition-html",
						"title": "GOVERN 3.1",
						"categories": "GOVERN-3",
						"url": " /govern-3/2012/01/16/team-composition.html",
						"content": "What is this subcategory about?\n  \nTo enhance organizational capacity for anticipating risks, AI actors should reflect a diversity of experience, expertise and backgrounds. Consultation with external personnel may be necessary when internal teams lack a diverse range of lived experiences or disciplinary expertise.\n\n  Research has shown that technicians with the same demographic backgrounds make similar misjudgments. To extend the benefits of diversity, equity, and inclusion to both the users and developers of it is recommended that teams are composed of a diverse group of individuals who reflect a range of backgrounds, perspectives and expertise. A diverse team enables more open sharing of ideas and assumptions about the AI system purpose and can help organizations understand:\n  \n    How the AI system may impact a wide variety of users;\n    How users might interact with the system;\n    Effectiveness of troubleshooting efforts, such as feedback channels; \nHow the system may create impacts beyond the intended users of the system.\n  \n\n  Without commitment from senior leadership, beneficial aspects of team diversity and inclusion can be overridden by unstated organizational incentives that inadvertently conflict with the broader values of a diverse workforce.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \nOrganizational management can:\n  \n    Define policies and hiring practices at the outset that promote interdisciplinary roles, competencies, skills, and capacity for AI efforts.\n    Define policies and hiring practices that lead to demographic and domain expertise diversity; empower staff with necessary resources and support, and facilitate the contribution of staff feedback and concerns without fear of reprisal.\n    Establish policies that facilitate inclusivity and the integration of new insights into existing practice.\n    Seek external expertise to supplement organizational diversity, equity, inclusion, and accessibility where internal expertise is lacking.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nDylan Walsh, “How can human-centered AI fight bias in machines and people?” MIT Sloan Mgmt. Rev., 2021, available at https://mitsloan.mit.edu/ideas-made-to-matter/howcan-human-centered-ai-fight-bias-machine.\n\n  Michael Li, “To Build Less-Biased AI, Hire a More Diverse Team,” Harvard Bus. Rev., 2020, available at https://hbr.org/2020/10/to-build-less-biased-ai-hire-a-more-diverse-team.\n\n  Bo Cowgill et al., “Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing\n\n  AI Ethics,” 2020, available at https://arxiv.org/pdf/2012.02394.pdf.\n\n  N. Ellemers and F. Rink, “Diversity in work groups,” Current opinion in psychology, vol. 11, pp. 49–53, 2016.\n\n  K. Talke, S. Salomo, and A. Kock, “Top management team diversity and strategic innovation orientation: The relationship and consequences for innovativeness and performance,” Journal of Product Innovation Management, vol. 28, pp. 819–832, 2011.\n\n  S. M. West, M. Whittaker, and K. Crawford, “Discriminating Systems: Gender, Race, and Power in AI,” AI Now Institute, Tech. Rep., 2019. [Online]. Available: https://ainowinstitute.org/discriminatingsystems.pdf\n\n  S. Fazelpour, M. De-Arteaga, Diversity in sociotechnical machine learning systems. Big Data &amp; Society. January 2022. doi:10.1177/20539517221082027\n\n  Cummings, M.L., and Li, S. 2021a. Sources of subjectivity in machine learning models. ACM Journal of Data and Information Quality, 13(2), 1–9"
					}
					
				
			
		
			
				
					,
					
					"govern-2-2012-01-16-executive-leadership-html": {
						"id": "govern-2-2012-01-16-executive-leadership-html",
						"title": "GOVERN 2.3",
						"categories": "GOVERN-2",
						"url": " /govern-2/2012/01/16/executive-leadership.html",
						"content": "What is this subcategory about?\n  \nSenior leadership in organizations that maintain an AI portfolio should be aware of AI risks and affirm the organizational appetite for such risks.\n\n  Accountability ensures that a specific team and individual is responsible for AI risk management efforts. Some organizations grant authority and resources (human and budgetary) to a designated officer who ensures adequate performance of the institution’s AI portfolio (e.g.  predictive modeling, machine learning).\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Organizational management can:\n      \n        Declare risk tolerances for developing or using AI systems.\n        Support AI risk management efforts, and play an active role in such efforts.\n        Support competent risk management executives\n      \n    \n    Large organizations can establish board committees for AI risk management and oversight functions and integrate those functions within the organization’s broader enterprise risk management approaches.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n  Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking Institutions, E-23 (Sept. 2017)."
					}
					
				
			
		
			
				
					,
					
					"govern-2-2012-01-16-education-and-training-html": {
						"id": "govern-2-2012-01-16-education-and-training-html",
						"title": "GOVERN 2.2",
						"categories": "GOVERN-2",
						"url": " /govern-2/2012/01/16/education-and-training.html",
						"content": "What is this subcategory about?\n  \nThrough regular training, AI actors should maintain awareness of:\n  \n    AI risk management goals and their role in achieving them.\n    organizational policies, applicable laws and regulations, and industry best practices and norms.\n  \n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish policies for personnel addressing ongoing education about:\n      \n        Applicable laws and regulations for AI systems.\n        Negative impacts that may arise from AI systems.\n        Organizational AI policies.\n        Trustworthy AI characteristics.\n      \n    \n    Ensure organizational AI policies include mechanisms for internal AI personnel to acknowledge and commit to their roles and responsibilities.\n    Ensure organizational policies address change management and include mechanisms to communicate and acknowledge substantial AI system changes.\n    Define paths along internal and external chains of accountability to escalate risk concerns.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html."
					}
					
				
			
		
			
				
					,
					
					"govern-2-2012-01-16-roles-and-responsibilities-html": {
						"id": "govern-2-2012-01-16-roles-and-responsibilities-html",
						"title": "GOVERN 2.1",
						"categories": "GOVERN-2",
						"url": " /govern-2/2012/01/16/roles-and-responsibilities.html",
						"content": "What is this subcategory about?\n  \nThe development of a risk-aware organizational culture starts with the definition of responsibilities. Under ideal risk management settings, oversight professionals are independent from model developers and report through risk management functions or directly to executives, countering implicit biases such as groupthink. This creates a firewall between technology development and risk management functions, so efforts cannot be easily bypassed or ignored.\n\n  There are numerous approaches for establishing strong risk management structures and tailoring them to the organizational risk profiles and resources. Facilitating a culture where AI system design and implementation decisions can be questioned and course-corrected by empowered stakeholders provides organizations with tools to anticipate and effectively manage risks before they materialize.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish policies that define the AI risk management roles and responsibilities for:\n      \n        Boards of directors or advisory committees\n        Senior management\n        AI audit functions\n        AI design\n        AI development\n        Human-AI interaction\n        AI testing and evaluation\n        AI acquisition and procurement\n        Impact assessment functions\n        Oversight functions\n      \n    \n    Establish policies that promote regular communication among teams participating in AI risk management efforts.\n    Establish policies that separate management of AI system development functions from AI system testing functions, to enable independent course-correction of AI systems.\n    Establish policies to prevent conflicts of interest, and counteract confirmation bias and market incentives that may hinder AI risk management efforts.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nAndrew Smith, “Using Artificial Intelligence and Algorithms,” FTC Business Blog (Apr. 8, 2020), available at https://www.ftc.gov/news-events/blogs/business-blog/2020/04/using-artificial-intelligence-algorithms.\n\n  Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking Institutions, E-23 (Sept. 2017).\n\n  Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011).\n\n  Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html."
					}
					
				
			
		
			
				
					,
					
					"govern-1-2012-01-16-documentation-html": {
						"id": "govern-1-2012-01-16-documentation-html",
						"title": "GOVERN 1.3",
						"categories": "GOVERN-1",
						"url": " /govern-1/2012/01/16/documentation.html",
						"content": "What is this subcategory about?\n  \nClear policies and procedures are necessary to communicate roles and responsibilities for the Map, Measure and Manage functions across the AI lifecycle.\n\n  Standardized documentation can operationalize how organizational AI risk management processes are implemented and recorded. Systematizing documentation can also enhance accountability efforts. By adding their contact information to a work product document, AI actors can improve communication, increase ownership of work products, and potentially enhance consideration of product quality. Documentation may generate downstream benefits related to improved system replicability and robustness. Proper documentation storage and access procedures allow for quick retrieval of critical information during a negative incident.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish and regularly review documentation policies that address information related to:\n      \n        AI actor contact information\n        Business justification\n        Scope and usage\n        Assumptions and limitations\n        Description of training data\n        Algorithmic methodology\n        Evaluated alternative approaches\n        Description of output data\n        Testing and validation results\n        Down- and up-stream dependencies\n        Plans for deployment, monitoring, and change management\n        Stakeholder engagement plans\n      \n    \n    Verify documentation policies for AI systems are standardized across the organization and up to date.\n    Establish policies for a model documentation inventory system and regularly review its completeness, usability, and efficacy.\n    Establish mechanisms to regularly review the efficacy of risk management processes.\n    Identify AI actors responsible for evaluating efficacy of risk management processes and approaches, and for course-correction based on results.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011).\n\n  Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://ww w.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management /index-model-risk-management.html.\n\n  Margaret Mitchell et al., “Model Cards for Model Reporting.” Proceedings of 2019 FATML Conference, available at https://arxiv.org/pdf/1810.03993.pdf.\n\n  Timnit Gebru et al., “Datasheets for Datasets,” Communications of the ACM 64, No. 12, 2021, available at https://arxiv.org/pdf/1803.09010.pdf.\n\n  Bender, E. M., Friedman, B. &amp; McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. .https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf\n\n  M. Arnold, R. K. E. Bellamy, M. Hind, et al. FactSheets: Increasing trust in AI services through supplier’s declarations of conformity. IBM Journal of Research and Development 63, 4/5 (July-September 2019), 6:1-6:13. https://ieeexplore.ieee.org/document/8843893\n\n  John Richards, David Piorkowski, Michael Hind, et al. A Human-Centered Methodology for Creating AI FactSheets. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering. Available at http://sites.computer.org/debull/A21dec/p47.pdf\n\n  OECD (2022), “OECD Framework for the Classification of AI systems”, OECD Digital Economy Papers, No. 323, OECD Publishing, Paris, https://doi.org/10.1787/cb6d9eca-en."
					}
					
				
			
		
			
				
					,
					
					"govern-1-2012-01-16-monitoring-and-review-html": {
						"id": "govern-1-2012-01-16-monitoring-and-review-html",
						"title": "GOVERN 1.2",
						"categories": "GOVERN-1",
						"url": " /govern-1/2012/01/16/monitoring-and-review.html",
						"content": "What is this subcategory about?\n  \nAI systems are dynamic and may perform in unexpected ways once deployed. Continuous monitoring is a risk management process for tracking unexpected issues and performance, in real-time or at a specific frequency, across the AI system lifecycle.\n\n  Incident response and “appeal and override” are commonly used processes in information technology management that are often overlooked for AI systems. These processes enable real-time flagging of potential incidents, and human adjudication of system outcomes.\n\n  Establishing and maintaining incident response plans can reduce the likelihood of additive impacts during an AI incident. Smaller organizations which may not have fulsome governance programs, can utilize incident response plans for addressing system failures, abuse and misuse.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \n    Establish policies and procedures for monitoring AI system performance, and to address bias and security problems, across the lifecycle of the system.\n    Establish policies for AI system incident response, or confirm that existing incident response policies address AI systems.\n    Establish policies to define organizational functions and personnel responsible for AI system monitoring and incident response activities.\n    Establish mechanisms to enable the sharing of feedback from impacted individuals or communities about negative impacts from AI systems.\n    Establish mechanisms to provide recourse for impacted individuals or communities to contest problematic AI system outcomes.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nNational Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. URL: https://nvlpubs. nist. gov/nistpubs/CSWP/NIST. CSWP, 4162018.\n\n  National Institute of Standards and Technology. (2012). Computer Security Incident Handling Guide. NIST Special Publication 800-61 Revision 2. URL: https://nvlpubs.nist.gov/nistpubs/specialpublications/nist.sp.800-61r2.pdf"
					}
					
				
			
		
			
				
					,
					
					"govern-1-2012-01-16-trustworthiness-embedded-into-processes-html": {
						"id": "govern-1-2012-01-16-trustworthiness-embedded-into-processes-html",
						"title": "GOVERN 1.1",
						"categories": "GOVERN-1",
						"url": " /govern-1/2012/01/16/trustworthiness-embedded-into-processes.html",
						"content": "What is this subcategory about?\n  \nPolicies, processes, and procedures are a central component of effective AI risk management and fundamental to individual and organizational accountability.\n\n  Organizational policies and procedures will vary based on available resources and risk profiles, but can help systematize AI actor roles and responsibilities throughout the AI model lifecycle. Without such policies, risk management can be subjective across the organization, and  exacerbate rather than minimize risks over time. \nIndividuals and organizations cannot be held accountable to unwritten, unknown or unrecognized policies. Lack of clear information about responsibilities and chains of command will limit the effectiveness of risk management.\n\n\n\n\n  How can organizations achieve the outcomes of this subcategory?\n  \nEstablish and maintain formal AI risk management policies that address AI system trustworthy characteristics throughout the system’s lifecycle. Organizational policies should:\n  \n    Define key terms and concepts related to AI systems and the scope of their intended use.\n    Address the use of sensitive or otherwise risky data.\n    Detail standards for experimental design, data quality, and model training.\n    Outline and document risk mapping and measurement processes and standards.\n    Detail model testing and validation processes.\n    Detail review processes for legal and risk functions.\n    Establish the frequency of and detail for monitoring, auditing and review processes.\n    Outline change management requirements.\n    Outline processes for internal and external stakeholder engagement.\n    Establish whistleblower policies to facilitate reporting of serious AI system concerns.\n    Detail and test incident response plans.\n    Verify that formal AI risk management policies align to existing legal standards, and industry best practices and norms.\n    Establish AI risk management policies that broadly align to AI system trustworthy characteristics.\n    Verify that formal AI risk management policies include currently deployed and third-party AI systems.\n  \n\n\n\n\n  What are the transparency and documentation considerations?\n  \nColumn G goes here.\n\n\n\n\n  What are some informative references?\n  \nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html.\n\n  GAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,” GAO@100 (GAO-21-519SP), June 2021, https://www.gao.gov/assets/gao-21-519sp.pdf.\n\n  NIST, “U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools” https://www.nist.gov/system/files/documents/2019/08/10/ai_standards_fedengagement_plan_9aug2019.pdf"
					}
					
				
			
		
	};
</script>
<script src="/RMF/js/lunr.min.js"></script>
<script src="/RMF/js/search.js"></script>

			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="copyright" style="text-align:center;">&copy; AI RMF Playbook Released August 2022. </p>
	</div>
</footer>



 		<script src="/RMF/js/accordion.js"></script>

	</body>
</html>
