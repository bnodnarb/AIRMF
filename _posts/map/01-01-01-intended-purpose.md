---
date: 01-01-01
title: MAP 1.1
categories:
  - MAP-1
description: Intended purpose, prospective settings in which the AI system will be deployed, the specific set or types of users along with their expectations, and impacts of system use are understood and documented. Assumptions and related limitations about AI system purpose and use are enumerated, documented and tied to TEVV considerations and system metrics.
type: Map
order_number: 1
---
{::options parse_block_html="true" /} 


<details>
<summary markdown="span">**About**</summary>      
<br>
It is not necessarily possible to have advanced knowledge about all potential settings in which a system will be deployed. To help delineate the bounds of acceptable deployment, context mapping may include examination of the following:
* intended, prospective,and actual deployment setting.
* specific set or types of users. 
* operator or subject expectations. 
* concept of operations. 
* intended purpose and impact of system use. 
* requirements for system deployment and operation. 
* potential negative impacts to individuals, groups, communities, organizations, and society – or context-specific impacts such as legal requirements or impacts to the environment. 
* unintended, downstream, or other unknown contextual factors.

</details>

<details>
<summary markdown="span">**Actions**</summary>

* Pursue AI system design purposefully, after non-AI solutions are considered. 
* Define and document the task, purpose, minimum functionality, and benefits of the AI system to inform considerations about whether the project is worth pursuing.
* Maintain awareness of industry, technical, and applicable legal standards.
* Collaboratively consider intended AI system design tasks along with unanticipated purposes.
* Determine the user and organizational requirements, including business and technical requirements.
* Determine and delineate the expected and acceptable AI system context of use, including:
    * operational environment
    * impacts to individuals, groups, communities, organizations, and society
    * user characteristics and tasks
    * social environment.
* Track and document existing AI systems held by the organization, and those maintained or supported by third-party entities.
* Gain and maintain awareness about evaluating scientific claims related to AI system performance and benefits before launching into system design.
* Identify human-AI interaction and/or roles, such as whether the application will support or replace human decision making. 
* Plan for risks related to human-AI configurations, and document requirements, roles, and responsibilities for human oversight of deployed systems.

</details>

<details>
<summary markdown="span">**Transparency and Documentation**</summary>         
                     
**Organizations can document the following:**
* Which AI actors are responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?
* Which AI actors are responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
* Who is the person(s) accountable for the ethical considerations across the AI lifecycle?

**AI Transparency Resources:**
* GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities
* “Stakeholders in Explainable AI,” Sep. 2018, [Online]. [link](http://arxiv.org/abs/1810.00184)
* "Microsoft Responsible AI Standard, v2" [link](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV)

</details>

<details>
<summary markdown="span">**References**</summary>      
<br>
**Socio-technical systems**

Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 59–68. [URL](https://doi.org/10.1145/3287560.3287598)

**Problem formulation**

Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. [URL](https://doi.org/10.1016/j.artint.2021.103555)

Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 39–48. [URL](https://doi.org/10.1145/3287560.3287567)

**Context mapping**

Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). [URL](https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en)

Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. [URL](https://arxiv.org/abs/2004.13676)

Social Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change Projects (Draft v2.0). [URL](https://www.alnap.org/system/files/content/resource/files/main/Draft%20SIMLab%20Context%20Analysis%20Framework%20v2.0.pdf)

Solon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. Commun. ACM 64, 7 (July 2021), 30–32. [URL](https://doi.org/10.1145/3466612)

**Identification of harms**

Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. [URL](https://arxiv.org/abs/1901.10002)

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. [URL](https://arxiv.org/abs/2011.13416)

Microsoft. Foundations of assessing harm. 2022. [URL](https://docs.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/harms-modeling/)

**Understanding and documenting limitations in ML**

Alexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. [URL](https://arxiv.org/abs/2011.03395)

Jessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. [URL](https://arxiv.org/abs/2205.08363)

Margaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220–229. [URL](https://doi.org/10.1145/3287560.3287596)

Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261. [URL](https://arxiv.org/abs/1808.07261)

Michael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1–14. [URL](https://doi.org/10.1145/3313831.3376445)

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. [URL](https://arxiv.org/abs/1803.09010)

Bender, E. M., Friedman, B. & McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. [URL](https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf)

Meta AI. System Cards, a new resource for understanding how AI systems work, 2021. [URL](https://ai.facebook.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/)

**When not to deploy**

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. [URL](https://doi.org/10.1145/3351095.3375691)

**Statistical balance**

Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. [URL](https://doi.org/10.1126/science.aax2342)

**Assessment of science in AI**

Arvind Narayanan. How to recognize AI snake oil. [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Emily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022). [URL](https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd)

</details>
